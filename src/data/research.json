[
    {
        "id": "research-area-1",
        "title": "3D Spatial AI & Autonomous Navigation",
        "icon": "Map",
        "shortDescription": "Enabling autonomous systems to perceive and navigate complex 3D environments.",
        "description": [
            {
                "title": "Overview",
                "contents": "Our research focuses on enabling autonomous systems to perceive, reconstruct, and navigate complex three-dimensional environments with human-level intelligence. We bridge the gap between geometric computer vision and modern deep learning to solve fundamental challenges in spatial AI."
            },
            {
                "title": "High-Fidelity 3D Reconstruction & Spatial Understanding",
                "contents": "We are advancing the state-of-the-art in 3D reconstruction by leveraging Generative AI and Gaussian Splatting technologies. Our research moves beyond traditional Structure-from-Motion (SfM) by developing robust algorithms capable of creating dense, photorealistic 3D models of large-scale outdoor environments, even in \"pose-free\" scenarios where camera location data is unavailable or unreliable. This technology allows autonomous agents to instantly generate high-precision digital twins of unknown areas for localization and mapping."
            },
            {
                "title": "End-to-End Autonomous Driving Perception",
                "contents": "We aim to transition autonomous driving systems from modular pipelines to unified, end-to-end learning architectures. Our work involves optimizing and lightweighting massive foundation models to ensure they are deployable on real-world robotic platforms without compromising performance. A key focus is 3D Occupancy Prediction and End-to-End Camera Pose Estimation, where we develop novel network structures using token-based approaches to accurately perceive scene geometry and dynamic objects, ensuring safe and efficient decision-making in crowded traffic environments."
            },
            {
                "title": "Multi-Modal Sensor Fusion & Calibration",
                "contents": "Robust perception relies on the precise integration of diverse sensor data. We possess core technologies in automatic extrinsic calibration for multi-modal sensor setups, including Camera-LiDAR and Camera-Radar systems. By utilizing geometric constraints such as vertical planes and point-line correspondences, we ensure sub-pixel accuracy in sensor alignment. This foundational work supports our advanced research in Semantic Scene Completion (SSC) and 3D Object Detection, enabling systems to infer and reconstruct occluded regions by fusing 2D semantic features with 3D sparse depth data."
            }
        ],
        "imageUrl": "src/assets/logo/AIS_logo.png"
    },
    {
        "id": "research-area-2",
        "title": "Advanced Visual Perception & Surveillance",
        "icon": "ScanEye",
        "shortDescription": "Robust object detection and understanding in dynamic, challenging conditions.",
        "description": [
            {
                "title": "Overview",
                "contents": "Our research pushes the boundaries of computer vision by developing robust algorithms that can perceive and understand dynamic environments. We focus on overcoming the limitations of traditional CNNs by integrating Transformer architectures, Spatio-Temporal fusion, and Label-Efficient learning strategies for intelligent surveillance systems."
            },
            {
                "title": "Next-Generation Transformer Detectors",
                "contents": "We are redefining object detection architectures by addressing the fundamental limitations of existing Transformer models (e.g., DETR). Our research introduces novel mechanisms such as Multi-Task Fusion Detectors (MTFD) that explicitly separate content classification from position regression to resolve task conflicts. We also developed Class Token Encoders that inject global category priors into the feature extraction stage, significantly enhancing the model's ability to distinguish objects in complex scenes. Further exploration includes applying Perceiver IO and Random Swin Transformers to dense prediction tasks like semantic segmentation."
            },
            {
                "title": "Spatio-Temporal Fusion for Robust Surveillance",
                "contents": "To achieve robust foreground detection in challenging surveillance environments (e.g., dynamic backgrounds, severe weather), we have developed advanced Spatio-Temporal Fusion Networks (STFN). Our approaches, such as MSF-NET, efficiently fuse spatial semantic features with temporal motion cues using split-and-merge frameworks. We also pioneer Spatio-Temporal Data Augmentation techniques that manipulate background and past frames to improve model generalization against static foregrounds and ghost effects."
            },
            {
                "title": "Weakly Supervised & Label-Efficient Learning",
                "contents": "Recognizing the high cost of annotating surveillance data, we focus on Weakly Supervised Learning (WSL) and Reinforcement Learning (RL). We have proposed an Actor-Critic based object detection method that learns to localize objects using only image-level counts (weak labels) instead of bounding boxes. Additionally, we developed domain adaptation frameworks that utilize generated background model images to adapt to new environments without requiring pixel-level foreground labels."
            },
            {
                "title": "Generative Background Modeling",
                "contents": "We leverage Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) to generate high-fidelity background models. By synthesizing clean background images from cluttered scenes, our methods significantly improve the performance of foreground detection algorithms, enabling them to handle abrupt lighting changes and camouflaged objects more effectively than classical statistical models."
            }
        ],
        "imageUrl": "src/assets/logo/AIS_logo.png"
    },
    {
        "id": "research-area-3",
        "title": "Scene Text Recognition (STR)",
        "icon": "Type",
        "shortDescription": "Recognizing irregular and occluded text in the wild using hybrid deep learning.",
        "description": [
            {
                "title": "Overview",
                "contents": "We address the challenge of reading irregular, curved, and occluded text in natural environments (\"text in the wild\"), which remains a difficult problem for traditional OCR systems. Our research focuses on developing hybrid deep learning architectures that combine the local feature extraction capabilities of CNNs with the global context understanding of Transformers."
            },
            {
                "title": "Multi-Branch Network Architectures",
                "contents": "To improve recognition accuracy for arbitrary-shaped text, we have developed novel \"Multi-Encoder\" and \"Multi-Decoder\" frameworks. By simultaneously utilizing a CNN-based encoder for local visual details and a Transformer-based encoder for global semantic context, our models capture richer feature representations. Furthermore, we employ a dual-decoder strategy that integrates Connectionist Temporal Classification (CTC) with Attention-based decoding mechanisms, allowing the model to mutually refine predictions and handle both alignment-free and sequence-dependent text effectively."
            },
            {
                "title": "Transformer-based Text Analysis",
                "contents": "Moving beyond fixed-size input constraints, we research \"Patch-based Transformer\" approaches where text images are sliced into vertical patches of varying scales. This method allows the network to robustly process text with extreme aspect ratios and perspective distortions. We also explore self-supervised learning techniques to reconstruct and recognize text in low-quality images affected by blur or severe noise."
            }
        ],
        "imageUrl": "src/assets/logo/AIS_logo.png"
    },
    {
        "id": "research-area-4",
        "title": "Fundamental Vision & RL Application",
        "icon": "Zap",
        "shortDescription": "Automating vision algorithms and optimization using Deep Reinforcement Learning.",
        "description": [
            {
                "title": "Overview",
                "contents": "We bridge the gap between classical computer vision theory and modern artificial intelligence by applying Deep Reinforcement Learning (DRL) to fundamental vision tasks. Our research focuses on automating the hyperparameter optimization processes that traditionally required manual tuning, transforming static algorithms into dynamic, intelligent agents."
            },
            {
                "title": "Adaptive Edge Detection via Reinforcement Learning",
                "contents": "We have developed a novel framework that optimizes the Canny Edge Detector using DRL. Instead of using fixed global thresholds, our agents—trained with algorithms ranging from Deep Q-Networks (DQN) to advanced Actor-Critic models—interact with image patches to adaptively predict the optimal hysteresis thresholds (high and low) for each specific local region. This approach significantly enhances edge continuity and suppresses noise in diverse lighting conditions compared to traditional methods."
            },
            {
                "title": "Weakly Supervised & Policy-Based Learning",
                "contents": "To overcome the dependency on pixel-perfect ground truth data, we research \"Weakly Supervised\" RL frameworks. Our latest work introduces methods to train edge detection agents using only weak labels, drastically reducing annotation costs while maintaining high precision. Furthermore, we extend RL to higher-level tasks, such as Policy-Based Object Detection, where we treat object localization as a sequential decision-making process, enabling the model to learn efficient search strategies in complex visual environments."
            }
        ],
        "imageUrl": "src/assets/logo/AIS_logo.png"
    },
    {
        "id": "research-area-5",
        "title": "Industrial Machine Vision",
        "icon": "Settings",
        "shortDescription": "High-precision, high-speed machine vision for smart factory automation.",
        "description": [
            {
                "title": "Overview",
                "contents": "We develop high-speed, high-precision Machine Vision algorithms that serve as the \"eyes\" of smart factories. Our research spans from traditional geometric image processing to modern Deep Learning-based defect detection, ensuring reliability in real-world manufacturing environments."
            },
            {
                "title": "High-Performance Automated Optical Inspection (AOI)",
                "contents": "To meet the demands of rapid production lines, we research optimization techniques for vision systems. Our recent work focuses on GPU-accelerated image processing architectures to significantly reduce processing time for high-resolution tasks, such as Optical Image Stabilization (OIS) module inspection. We also apply Convolutional Neural Networks (CNNs) to automate defect detection in complex patterns like solar cells, surpassing the accuracy limitations of traditional rule-based algorithms."
            },
            {
                "title": "3D Metrology & Vision-Guided Robotics",
                "contents": "We develop non-contact 3D measurement systems using structured light (e.g., slit lasers) to precisely reconstruct the shape of industrial components like fasteners. Furthermore, we integrate 3D vision with robotic manipulators, enabling fully automated processes such as the precise handling and assembly of automotive parts (e.g., tie rods) based on real-time visual feedback."
            },
            {
                "title": "Robust Geometric Inspection Algorithms",
                "contents": "Our lab possesses extensive expertise in developing robust algorithms for verifying the geometry of diverse industrial parts. We have solved practical engineering challenges, including detecting protrusions in coils, inspecting diamond wheels via edge extraction, and verifying screw heads (Torx) using morphological processing. We also specialize in robust pattern recognition, such as decoding Data Matrix codes under severe perspective distortion."
            }
        ],
        "imageUrl": "src/assets/logo/AIS_logo.png"
    }
]