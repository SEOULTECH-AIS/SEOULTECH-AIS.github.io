@article{choi2024object,
  title = {Object {{Detection Method Using Image}} and {{Number}} of {{Objects}} on {{Image}} as {{Label}}},
  author = {Choi, Keong-Hun and Ha, Jong-Eun},
  year = {2024},
  journal = {IEEE Access},
  volume = {12},
  pages = {121915--121931},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2024.3452728},
  urldate = {2025-10-10},
  abstract = {Object detection is an essential step in various applications. After deep learning appeared, convolutional neural networks or transformers have shown significant improvement in object detection compared to statistically motivated algorithms. But, they still require improvement in multiple aspects. One is to maintain detection performance in unseen environments without retraining using images and labels from unseen environments. The other is to reduce the tight requirements of labels. In object detection, a bounding box is usually used as a label for an object. In this paper, we propose an object detection algorithm that requires only images and the number of objects on images as labels. We approach the problem with deep reinforcement learning. The proposed algorithm uses an actor-critic algorithm that can produce continuous action. We make an actor model to produce multiple bounding boxes, and a critic model evaluates well as training goes on. Also, we propose a reward model using a pre-trained model trained with an object detection dataset. The proposed algorithm requires only images and the number of objects on images, not bounding boxes. We show that the proposed algorithm gives a comparable result to the transformer-based approach through experiments. Also, it can adapt to unseen environments by only using images and the number of objects on images.},
  keywords = {actor-critic,Convolutional neural networks,deep reinforcement learning,Deep reinforcement learning,Image analysis,Labeling,Object detection,PPO,Proposals,Standards,Training,Transformers}
}

@article{aninmo2011eji,
  title = {{에지 투영과 PCA를 이용한 차대 번호 인식}},
  author = {안인모 and 하종은},
  year = {2011},
  month = may,
  journal = {제어로봇시스템학회 논문지},
  volume = {17},
  number = {5},
  pages = {479--483},
  issn = {1976-5622},
  urldate = {2025-10-04},
  abstract = {The automation of production process is actively expanding for the purpose of the cost reduction and quality assurance. Among these, automatic tracking of the product along the whole process of the production is also important topic. Typically this is done by adopting OCR technology. Conventional OCR technology operates well on the rather good quality of the image like as printed characters on the paper. In industrial application, IDs are marked on the metal surface, and this cause the height difference between background material and character. Illumination systems that guarantee an image with good quality may be a solution, but it is rather difficult to design such an illumination system. This paper proposes an algorithm for the recognition of vehicle' ID characters using edge projection and PCA (Principal Component Analysis). Proposed algorithm robustly operates under illumination change using the same parameters. Experimental results show the feasibility of the proposed algorithm.},
  langid = {korean},
  keywords = {OCR,segmentation,vehicle ID}
}

@article{bagceolhong2023mask2former,
  title = {{Mask2Former 를 이용한 크랙 시맨틱 분할}},
  author = {박철홍 and 하종은},
  year = {2023},
  month = dec,
  journal = {제어로봇시스템학회 논문지},
  volume = {29},
  number = {12},
  pages = {1039--1045},
  issn = {1976-5622},
  doi = {10.5302/J.ICROS.2023.23.0145},
  urldate = {2025-10-03},
  abstract = {The objective of crack detection is to identify any defects present on the surfaces of various physical structures. This task can be approached in two ways: bounding box detection and semantic segmentation. In this study, we focus on a method based on semantic segmentation that can provide per-pixel classification results. We applied Mask2Former, a method known for its state-of-theart performance in semantic segmentation, for crack detection. We conducted experiments using various crack datasets, and the results highlight the need for enhanced performance in non-crack detection to achieve improved results.},
  langid = {korean},
  keywords = {crack,deep learning,semantic segmentation,transformer}
}

@article{bagsangmin2023dajung,
  title = {{다중 특징치 맵과 마스크드 오토인코드를 이용한 3D 시맨틱 장면 완성}},
  author = {박상민 and 하종은},
  year = {2023},
  month = dec,
  journal = {제어로봇시스템학회 논문지},
  volume = {29},
  number = {12},
  pages = {966--972},
  issn = {1976-5622},
  doi = {10.5302/J.ICROS.2023.23.0143},
  urldate = {2025-10-03},
  abstract = {Autonomous systems require a profound understanding of their surroundings, encompassing both semantic and 3D geometry. This study focuses on advancing 3D semantic scene completion approaches using a camera. Building upon the foundation laid by VoxFormer [1], which is recognized for its state-of-the-art performance in 3D semantic scene completion, our approach involves two distinct stages. In the initial stage, scene completion is done with depth images, while in the second stage, the final 3D scene completion is performed using masked autoencoder. To enhance the performance of VoxFormer, we introduced two key modifications. First, we modified the first stage using multi-scale feature maps. Second, we further modified the first stage using a masked autoencoder. Experimental results, based on the adapted VoxFormer model in both stages are presented. Our two proposed approaches exhibit notable improvements, particularly in the context of small objects. However, these enhancements warrant further investigation for optimization and refinement.},
  langid = {korean},
  keywords = {deep learning,scene completion,scene understanding,semantic segmentation}
}

@article{choi2021adaptive,
  title = {An {{Adaptive Threshold}} for the {{Canny Algorithm With Deep Reinforcement Learning}}},
  author = {Choi, Keong-Hun and Ha, Jong-Eun},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {156846--156856},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3130132},
  urldate = {2025-10-03},
  abstract = {The Canny algorithm is widely used for edge detection. It requires the adjustment of parameters to obtain a high-quality edge image. Several methods can select them automatically, but they cannot cover the diverse variations on an image. In the Canny algorithm, we need to set values of three parameters. One is related to smoothing window size, and the other two are the low and high threshold. In this paper, we assume that the smoothing window size is fixed to a predefined size. This paper proposes a method to provide adaptive thresholds for the Canny algorithm, which operates well on images acquired under various variations. We select optimal values of two thresholds adaptively using an algorithm based on the Deep Q-Network (DQN). We introduce a state model, a policy model, and a reward model to formulate the given problem in deep reinforcement learning. The proposed method has the advantage that it can adapt to a new environment using only images without labels, unlike the existing supervised way. We show the feasibility of the proposed algorithm by diverse experimental results.},
  keywords = {Adaptation models,deep Q-network,deep reinforcement learning,Edge detection,Games,Hysteresis,Image edge detection,Reinforcement learning,Smoothing methods,Training}
}

@article{choi2023adaptive,
  title = {An {{Adaptive Threshold}} for the {{Canny Edge With Actor-Critic Algorithm}}},
  author = {Choi, Keong-Hun and Ha, Jong-Eun},
  year = {2023},
  journal = {IEEE Access},
  volume = {11},
  pages = {67058--67069},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2023.3291593},
  urldate = {2025-10-03},
  abstract = {We propose a method to automatically select proper values of three thresholds in the Canny edge algorithm. Edge detection is widely used for object recognition, detection, and segmentation. Due to its good performance, the Canny edge algorithm is still widely used among many edge detection algorithms. But, it requires manually selecting three appropriate thresholds for the given image. Some approaches have been proposed for automatically setting thresholds in the Canny edge algorithm. But, they either deal with partial among three entries or only show their performance in a limited range of variation. In natural scenes, images are acquired under various illumination, pose, and weather conditions. This paper proposes a method that can operate in various environments. We formulate the given problem by adopting an actor-critic algorithm. We propose an actor and critic network to solve the problem with an actor-critic algorithm. Also, we suggest a reward configuration based on an edge evaluation network and measure to prevent the reversal between high and low thresholds. The edge evaluation network uses an original image and an edge image as input. We set a negative reward when reversing the high and low thresholds occur. The proposed algorithm can adapt to unseen environments using images without requiring ground truth labels. Experimental results using diverse datasets show the feasibility of the proposed algorithm.},
  keywords = {Actor-critic algorithm,Classification algorithms,deep learning,Deep learning,deep reinforcement learning,edge detection,Filtering algorithms,Image edge detection,Reinforcement learning,Smoothing methods,Training}
}

@article{choi2023improved,
  title = {Improved {{Image Classification With Token Fusion}}},
  author = {Choi, Keong-Hun and Kim, Jin-Woo and Wang, Yao and Ha, Jong-Eun},
  year = {2023},
  journal = {IEEE Access},
  volume = {11},
  pages = {67460--67467},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2023.3291597},
  urldate = {2025-10-03},
  abstract = {In this paper, we propose a method to improve image classification performance using the fusion of CNN and transformer structure. In the case of CNN, information about a local area on an image can be extracted well, but global information extraction is limited. On the other hand, the transformer has an advantage in global information extraction, but it requires much memory compared to CNN. We apply CNN on an image and consider the feature vector of each pixel on the resulting feature map by CNN as a token. At the same time, the image is divided into patches, and each patch is considered a token, like a transformer. Tokens by CNN and transformer have advantages in extracting local and global information, respectively. We assume that the combination of these two types of tokens will have an improved characteristic, and we show it through experiments. We propose three methods to fuse tokens having different characteristics: (1) late token fusion with parallel structure, (2) early token fusion (3) token fusion in layer-by-layer. The proposed method shows the best classification performance in experiments using ImageNet-1K.},
  keywords = {convolutional neural networks,Convolutional neural networks,Data mining,deep learning,Deep learning,Feature extraction,Image classification,transformer,Transformers}
}

@article{coegyeonghun2019eji,
  title = {{에지 분류 CNN 을 이용한 U-Net 기반 에지 검출}},
  author = {최경훈 and 하종은},
  year = {2019},
  month = aug,
  journal = {제어로봇시스템학회 논문지},
  volume = {25},
  number = {8},
  pages = {684--689},
  issn = {1976-5622},
  doi = {10.5302/J.ICROS.2019.19.0119},
  urldate = {2025-10-04},
  abstract = {Edge detection is the first necessary step in image processing for object segmentation, detection, and recognition. The Canny algorithm is widely used filter-based approach, but it requires the correct adjustment of its parameters according to the variations in images. In this paper, we propose a method that is consisted of two steps for the robust detection of edges in an image. The proposed algorithm adopts convolutional neural networks that can handle the diverse variations caused by illumination, pose, and scale change. First, we train a convolutional neural network to decide whether a given input edge image is good or not. We can generate as many training images as we want using this network. Finally, U-Net is used to generate an edge image using a gray image as input. Experimental results show the robustness of the proposed algorithm for images acquired under outdoor and indoor environments.},
  langid = {korean},
  keywords = {convolutional neural networks,deep learning,edge}
}

@article{coegyeonghun2020cnn,
  title = {{CNN 을 이용한 태양전지 불량 검출}},
  author = {최경훈 and 하종은},
  year = {2020},
  month = dec,
  journal = {제어로봇시스템학회 논문지},
  volume = {26},
  number = {12},
  pages = {1093--1098},
  issn = {1976-5622},
  doi = {10.5302/J.ICROS.2020.20.0090},
  urldate = {2025-10-04},
  abstract = {The efficiency of solar panel power production is greatly influenced by the state of the panel. Therefore, it can be said that it is an important factor to check whether a solar cell is damaged in managing solar power. Existing solar cell damage detection technology detects whether the solar cell is damaged by an electroluminescence phenomenon or by checking the degree of scattering by shining a laser. In this study, we attempt to detect the damage of the solar cell by using the deep learning method and taking the surface image of the solar cell without additional measuring device. When using the existing electroluminescence phenomenon, a separate device must be configured to check for damage, and there is a disadvantage in that the use of the connected cell must be stopped to confirm a part. The method of checking the degree of scattering by illuminating the laser has a disadvantage that many areas cannot be seen at once. To improve this, this paper presents a detection method based on deep learning. We present an algorithm that is based on CNN (Convolutional Neural Network). Experimental results according to model size and data augmentation are given to show the feasibility of proposed method.},
  langid = {korean},
  keywords = {convolutional neural network,deep learning,defects detection,solar cell}
}

@article{coegyeonghun2020eji,
  title = {{에지 비용 함수를 이용한 시맨틱 분할}},
  author = {최경훈 and 하종은},
  year = {2020},
  month = nov,
  journal = {제어로봇시스템학회 논문지},
  volume = {26},
  number = {11},
  pages = {916--921},
  issn = {1976-5622},
  doi = {10.5302/J.ICROS.2020.20.0119},
  urldate = {2025-10-04},
  abstract = {Semantic segmentation aims to assign correct class labels per pixel on an image. In particular, semantic segmentation has difficulties in particular along the boundary of objects. Recently, ELKPPNet has been proposed, which improves the performance of semantic segmentation by adding edge loss term into the conventional semantic segmentation algorithm. In extracts edge from the end of the network, which is used in the computation of loss. In this paper, we present U-Net based networks which adopt the edge loss of the ELKPPNet. Presented algorithm computes edge by additional network flow in the U-Net. Two different network structures are investigated. One computes edge at the end of decoder in encoder-decoder of the U-Net. The other computes edge from the start of decoder in U-Net. Experimental results show that integrating edge information in semantic segmentation improves performance.},
  langid = {korean},
  keywords = {Convolutional Neural Networks,Deep Learning,Edge,Semantic Segmentation}
}

@article{coegyeonghun2021muin,
  title = {{무인 감시 Transformer}},
  author = {최경훈 and 하종은},
  year = {2021},
  month = dec,
  journal = {제어로봇시스템학회 논문지},
  volume = {27},
  number = {12},
  pages = {972--977},
  issn = {1976-5622},
  doi = {10.5302/J.ICROS.2021.21.0143},
  urldate = {2025-10-03},
  abstract = {In a visual surveillance system, even the same object should exhibit different detection results depending on the surrounding environment configuration. To this end, the model for visual surveillance needs to detect an object by understanding the state of the object according to the environment on the image. In this study, for such visual surveillance, an object segmentation model applied with a transformer structure suitable for image processing was used to divide objects inside the image into foreground and background. A modified attention structure was presented for the corresponding transformer structure, and the results of object segmentation models according to the type of input data were compared.},
  langid = {korean},
  keywords = {Deep learning,Segmentation,Transformer,Visual surveillance}
}

@article{coegyeonghun2021taegeu,
  title = {{태그 라벨과 트랜스포머를 이용한 시맨틱 분할}},
  author = {최경훈 and 하종은},
  year = {2021},
  month = dec,
  journal = {제어로봇시스템학회 논문지},
  volume = {27},
  number = {12},
  pages = {1023--1028},
  issn = {1976-5622},
  doi = {10.5302/J.ICROS.2021.21.0153},
  urldate = {2025-10-03},
  abstract = {Currently, research on artificial intelligence that autonomously interacts with surroundings without human management has attracted research attention in vehicle and robot-related fields. The recognition of the surrounding environment is the basis for artificial intelligence that requires interaction with the surroundings, which means that research on object detection is necessary. In general, object detection proceeds in the same way as detection and segmentation. Among them, in the case of segmentation, the size of the model is smaller, and more information can be obtained than detection using anchors. However, the inferior detection performance and generalization ability of this method for small objects has limited its further application. In this paper, a modified transformer structure with different configuration of training data from the existing label data is presented to improve the performance of segmentation.},
  langid = {korean},
  keywords = {Deep learning,Semantic Segmentation,Transformer}
}

@article{coegyeonghun2022gaegce,
  title = {{객체 검출과 객체 분할 방법의 무인 감시 데이터셋 적용 결과 비교}},
  author = {최경훈 and 하종은},
  year = {2022},
  month = oct,
  journal = {제어로봇시스템학회 논문지},
  volume = {28},
  number = {10},
  pages = {848--854},
  issn = {1976-5622},
  doi = {10.5302/J.ICROS.2022.22.0106},
  urldate = {2025-10-03},
  abstract = {Visual surveillance aims to detect foreground objects stably under diverse variations caused by weather changes. Traditional visual surveillance algorithms are based on a statistical analysis of pixel variations along the spatio-temporal domain. Recently, deep learning-based algorithms have improved performance compared to conventional algorithms. In this paper, we investigate the performance of object detection and semantic segmentation algorithms on visual surveillance datasets. We use the CDnet dataset, which is widely used in visual surveillance. We adjust labels in the CDnet dataset to be suitable for object detection and semantic segmentation. We investigate the possibility of object detection and semantic segmentation in visual surveillance. Two types of algorithms based on CNN and transformer are used in experiments. Experimental results show that spatio-temporal processing is required to improve performance when we apply object detection and semantic segmentation in visual surveillance.},
  langid = {korean},
  keywords = {Deep learning,Object detection,Semantic segmentation,Transformer,Visual surveillance}
}

@article{coeisag2011sogdo,
  title = {{속도 가변을 통한 영상교시 기반 주행 알고리듬 성능 향상}},
  author = {최이삭 and 하종은},
  year = {2011},
  month = apr,
  journal = {제어로봇시스템학회 논문지},
  volume = {17},
  number = {4},
  pages = {375--381},
  issn = {1976-5622},
  urldate = {2025-10-04},
  abstract = {This paper deals with the improvement of visual path following through velocity variation according to the coordinate of feature points. Visual path follow first teaches driving path by selecting milestone images then follows the route by comparing the milestone image and current image. We follow the visual path following algorithm of Chen and Birchfield [8]. In [8], they use fixed translational and rotational velocity. We propose an algorithm that uses different translational velocity according to the driving condition. Translational velocity is adjusted according to the variation of the coordinate of feature points on image. Experimental results including diverse indoor cases show the feasibility of the proposed algorithm.},
  langid = {korean},
  keywords = {autonomous navigation,feature matching,intelligent robot,path follow}
}

@article{coeisag2011yeongsang,
  title = {{영상 교시기반 주행 알고리듬 성능 평가}},
  author = {최이삭 and 하종은},
  year = {2011},
  month = sep,
  journal = {제어로봇시스템학회 논문지},
  volume = {17},
  number = {9},
  pages = {902--907},
  issn = {1976-5622},
  urldate = {2025-10-04},
  abstract = {In this paper, we deal with performance evaluation of visual path following using 2D and 3D information. Visual path follow first teaches driving path by selecting milestone images then follows the same route by comparing the milestone image and current image. We follow the visual path following algorithm of [8] and [10]. In [8], a robot navigated with 2D image information only. But in [10], local 3D geometries are reconstructed between the milestone images in order to achieve fast feature prediction which allows the recovery from tracking failures. Experimental results including diverse indoor cases show performance of each algorithm.},
  langid = {korean},
  keywords = {autonomous navigation,feature matching,intelligent robot,path follow}
}

@article{gimdaehun2017cnn,
  title = {{CNN 과 전이 학습을 이용한 다차선 검출}},
  author = {김대훈 and 하종은},
  year = {2017},
  month = sep,
  journal = {제어로봇시스템학회 논문지},
  volume = {23},
  number = {9},
  pages = {718--724},
  issn = {1976-5622},
  doi = {10.5302/J.ICROS.2017.17.0107},
  urldate = {2025-10-04},
  abstract = {Multi-lane detection is essential in autonomous navigation. Conventional algorithms have difficulties due to diverse image variations caused by illumination variations, occlusions, and shadows. Recently, deep-learning algorithms which use large amounts of training data show dramatic improvements in many areas. In this paper, we address multi-lane detection using convolution neural networks (CNNs) and transfer learning. The CNNs' architectures are used for the detection of multiple lanes, and transfer the learning using pre-learned models from ImageNet to reduce the learning time. A sliding window is used to detect lane candidates on an image and it requires heavy computation time. We present a method to reduce the detection time by changing the input structure when applying trained networks. Images from KITTI are used for training. Experiments are conducted applying trained networks to images obtained in other environments.},
  langid = {korean},
  keywords = {convolutional neural networks,deep learning,lane detection,transfer learning}
}

@article{gimdaehun2018cnn,
  title = {{CNN 소실점 검출을 이용한 차선 검출}},
  author = {김대훈 and 하종은},
  year = {2018},
  month = sep,
  journal = {제어로봇시스템학회 논문지},
  volume = {24},
  number = {9},
  pages = {851--856},
  issn = {1976-5622},
  doi = {10.5302/J.ICROS.2018.18.0111},
  urldate = {2025-10-04},
  abstract = {Lane detection is essential in autonomous navigation. Conventional algorithms use hand crafted features which produce difficulties because of diverse image variations from illumination variations, occlusions and shadows. Recently, deep learning based approaches have provided more robust results. In this paper, we present an algorithm for the robust detection of lanes by finding vanishing points with convolutional neural networks. We use two modified CNN architectures, where the final output layer consists of four elements. The epipole and the angles of the current driving lane each have two elements. Experiments are performed by using two modified structures of the NVIDIA end-to-end model[9] and the ResNet-50 model[10].},
  langid = {korean},
  keywords = {convolutional neural networks,deep learning,lane detection,transfer learning}
}

@article{gimdaehun2018robos,
  title = {{로봇 물체 조작을 위한 딥러닝 시맨틱 분할을 이용한 물체 검출}},
  author = {김대훈 and 하종은},
  year = {2018},
  month = sep,
  journal = {제어로봇시스템학회 논문지},
  volume = {24},
  number = {9},
  pages = {802--808},
  issn = {1976-5622},
  doi = {10.5302/J.ICROS.2018.18.0101},
  urldate = {2025-10-04},
  abstract = {Object detection is the first necessary step for the manipulation of objects with robot. Object detection is used to find the type and number of a specific object on an image and identify the place where the object is located. In this paper, we adopt semantic segmentation method for the robust detection of objects in an image. We use the FCN[6] architecture which consists of an encoder and decoder for the semantic segmentation. Transfer learning is adopted where the parameters of the encoder part are used as the one which are learned by ImageNet and the parameters of the decoder part are newly learned by using training images of given tasks. We deal with three classes including two objects and one background for the semantic segmentation. The experimental results show the feasibility of the presented algorithm.},
  langid = {korean},
  keywords = {convolutional neural network,deep learning,object detection,semantic segmentation}
}

@article{gimdaehun2019gaeyi,
  title = {{두 개의 이종 CNN 융합을 이용한 차선 검출}},
  author = {김대훈 and 하종은},
  year = {2019},
  month = sep,
  journal = {제어로봇시스템학회 논문지},
  volume = {25},
  number = {9},
  pages = {753--759},
  issn = {1976-5622},
  doi = {10.5302/J.ICROS.2019.19.0110},
  urldate = {2025-10-04},
  abstract = {Lane detection is essential in many applications including autonomous navigation and intelligent vehicles. Recently, the performance of image recognition and detection has been remarkably improved by Convolutional Neural Networks (CNN). In this paper, we present a method for lane detection by combining the results of two CNN architectures. The first CNN detects lane locations on the image via a sliding window, while the second one detects the vanishing point and the lane angle. By combining the results of these two structures, we present a method to improve lane detection results by comparing the lane detection result from each structure.},
  langid = {korean},
  keywords = {convolutional neural networks,deep learning,lane detection,transfer learning}
}

@article{gimjaeyeol2019jeogoeseon,
  title = {{적외선 차단 필터를 제거한 더미 카메라를 이용한 카메라와 라이다 외부 보정 방법 평가}},
  author = {김재열 and 하종은},
  year = {2019},
  month = jul,
  journal = {제어로봇시스템학회 논문지},
  volume = {25},
  number = {7},
  pages = {597--602},
  issn = {1976-5622},
  doi = {10.5302/J.ICROS.2019.19.0080},
  urldate = {2025-10-04},
  abstract = {In this paper, we present the evaluation results of the extrinsic calibration of a camera and LiDAR using a dummy camera without an IR-cut filter. In general, an IR-cut filter is included in a camera to remove the effect of the IR band. If the IR-cut filter is removed from the camera, the true locus of LiDAR could be observed on the image. We evaluated two algorithms that are widely used for the extrinsic calibration of a camera and LiDAR by using the true locus of LiDAR on the dummy camera. After the extrinsic calibration between the camera and LiDAR, we projected the LiDAR data onto the dummy camera using the pose between two cameras. Finally, we evaluated the results of the extrinsic calibration by comparing the projected locus with the true locus. The existing algorithms evaluate the calibration results only by checking the residuals of the used constraints or by simply checking the consistency at the depth discontinuities. In this paper, we present the precise evaluation results obtained using the true locus of the LiDAR.},
  langid = {korean}
}

@article{gimjaeyeol2020vaero,
  title = {{VAE로 생성한 배경 이미지를 이용한 전경 물체 분할}},
  author = {김재열 and 하종은},
  year = {2020},
  month = nov,
  journal = {제어로봇시스템학회 논문지},
  volume = {26},
  number = {11},
  pages = {964--970},
  issn = {1976-5622},
  doi = {10.5302/J.ICROS.2020.20.0118},
  urldate = {2025-10-04},
  abstract = {In visual surveillance, the robust detection of foreground objects under diverse environmental changes is the main goal. In the case of traditional algorithms, they usually obtain a background model image through the statistical analysis, which is used for finding foreground objects by comparing with a current image. Recently, many deep learning-based visual surveillance algorithms have been proposed, and they show improved performance than traditional algorithms. However, they usually show a good performance when test images are similar to training environments. Retraining is required to have an improved result in scenes which are different from training environments. In this paper, we aim to have an improved deep learning-based visual surveillance algorithm which also an improvement in new scenes. We use two types of images as the input of the U-Net, which produces a foreground segmentation map as output. A background model image which is generated by VAE is used one type of input. The other type of input to the network is multiple original images. Also, we train the presented network using multiple scenes while most conventional deep learning-based visual surveillance algorithms newly train a network per scene. Experimental results using various open datasets show the feasibility of the presented algorithm.},
  langid = {korean},
  keywords = {deep learning,Foreground objects detection,segmentation,VAE,visual surveillance}
}

@article{gimjinu2015measurement,
  title = {{Measurement of 3D Shape of Fastener using Camera and Slit Laser}},
  author = {김진우 and 송태훈 and 하종은},
  year = {2015},
  month = jun,
  journal = {Journal of the Korean Society for Precision Engineering},
  volume = {32},
  number = {6},
  pages = {537--542},
  issn = {1225-9071},
  urldate = {2025-10-04},
  abstract = {The measurement of 3D shape is important in inspecting the quality of product. In this paper, we present a 3D shape measurement system of fastener using a camera and a slit laser. Calibration structure with slits is used in the extrinsic calibration of the camera and laser. The pose of the camera and laser is computed under the same world coordinate system in the calibration structure. Reflection of laser light on the metal surface causes many difficulties in the robust detection of them on image. We overcome this difficulty by using color and dynamic programming. Motor stage is used to rotate the fastener to recover the whole 3D shape of the surface of it.},
  langid = {korean},
  keywords = {3D shape measurement (3D ),Camera calibration ( ),Extrinsic calibration ( ),Machine vision ( )}
}

@article{gimjinu2017danan,
  title = {{단안 카메라를 이용한 소형 자동차의 임무 수행}},
  author = {김진우 and 하종은},
  year = {2017},
  month = jan,
  journal = {한국전자통신학회 논문지},
  volume = {12},
  number = {1},
  pages = {123--128},
  issn = {1975-8170},
  urldate = {2025-10-04},
  abstract = {본 논문에서는 카메라 및 기타 센서 처리를 통한 소형차의 자율 주행을 통한 미션 수행에 대해 다루도록 한다. 주어진 차선내에서 안전한 운행을 위해서는 차선내에서의 차량의 자세 정보 추출이 필요하다. 이를 위해 호모그라피를 이용하도록 한다. 주어진 영상을 흑백 이미지로 변환후 이치화와 에지를 이용하여 차선에서 필요한 제어점들을 추출한다. 호모그라피를 이용하여 두 개의 제어점들을 세계 좌표계로 변환후 차량의 각도와 위치를 계산하도록 한다. 칼라 정보를 이용하여 신호등 판단을 하도록 한다. 실험을 통해 주어진 임무를 잘 수행함을 확인할 수 있었다.},
  langid = {korean},
  keywords = {,Color Processing,Localization,Plane Homography,Robot Vision}
}

@article{gimjinu2022teugjingci,
  title = {{특징치 플립을 이용한 이미지 분류 모델}},
  author = {김진우 and 하종은},
  year = {2022},
  month = oct,
  journal = {제어로봇시스템학회 논문지},
  volume = {28},
  number = {10},
  pages = {903--909},
  issn = {1976-5622},
  doi = {10.5302/J.ICROS.2022.22.0108},
  urldate = {2025-10-03},
  abstract = {After Transformer and ViT, the use of attention in vision is increasing, and the performance has also surpassed the existing convolution layer. Attention uses a linear layer by default. Therefore, attention can be seen as an improvement of the linear layer. The direction of improvement of linear and convolution layers is similar when looking at the contents studied so far. The biggest difference between the two is the size of the receptive field. In this paper, various types of FlipConv blocks are presented. In particular, FlipConv block type4 shows similar performance to ConvNeXt block with a small parameter increase. And it shows that the performance of about 8\% is good for the data in the form not used for learning.},
  langid = {korean},
  keywords = {Backbone,Classification,Deep learning,Feature map,Flip}
}

@article{gimnamhun2016yeongsang,
  title = {{영상 처리를 이용한 소형 이족 보행 로봇의 임무 수행}},
  author = {김남훈 and 하종은},
  year = {2016},
  month = dec,
  journal = {한국전자통신학회 논문지},
  volume = {11},
  number = {12},
  pages = {1225--1230},
  issn = {1975-8170},
  urldate = {2025-10-04},
  abstract = {본 논문에서는 영상 처리를 이용한 소형 이족 로봇의 임무 수행에 대해 다루도록 한다. 주어진 지도상에서 안정적인 보행을 위해서는 위치 인식이 필요하게 된다. 주행 경로상의 경계를 추출하여 로봇의 위치 정보를 추정하도록 한다. 평면상에서 운동하는 경우 위치와 방향으로 이루어진 세 개의 인자의 추정이 필요하다. 본 논문에서는 주행 경로상의 수직 방향의 로봇의 위치와 진행 방향등 두 개의 인자를 추정하여 위치 추정을 하도록 한다. 장애물 검출을 위해서는 칼라 정보를 이용하도록 하며 안정적인 검출을 위해 정규화된 값을 이용하도록 한다.},
  langid = {korean},
  keywords = {,Biped Walking Robot,Localization,Robot Vision}
}

@article{gimseseon2009seutereo,
  title = {{스테레오 기반의 장애물 회피 알고리듬}},
  author = {김세선 and 김현수 and 하종은},
  year = {2009},
  month = jan,
  journal = {제어로봇시스템학회 논문지},
  volume = {15},
  number = {1},
  pages = {89--93},
  issn = {1976-5622},
  urldate = {2025-10-05},
  abstract = {This paper deals with obstacle avoidance for unmanned vehicle using stereo system. The ``DARPA Grand Challenge 2005'' shows that the robot can move autonomously under given waypoint. RADAR, IMS (Inertial Measurement System), GPS, camera are used for autonomous navigation. In this paper, we focus on stereo system for autonomous navigation. Our approach is based on Singh et. al. [5]'s approach that is successfully used in an unmanned vehicle and a planetary robot. We propose an improved algorithm for obstacle avoidance by modifying the cost function of Singh et. al. [5]. Proposed algorithm gives more sharp contrast in choosing local path for obstacle avoidance and it is verified in experimental results.},
  langid = {korean},
  keywords = {obstacle avoidance,stereo vision,traversability,unmanned vehicle}
}

@article{ha2000new,
  title = {A New Calibration Algorithm Using Known Angles},
  author = {Ha, J. E. and Kweon, I. S.},
  year = {2000},
  journal = {Electronics Letters},
  volume = {36},
  number = {1},
  pages = {20--22}
}

@article{ha2000robust,
  title = {Robust Direct Motion Estimation Considering Discontinuity},
  author = {Ha, Jong-Eun and Kweon, In-So},
  year = {2000},
  month = oct,
  journal = {Pattern Recognition Letters},
  volume = {21},
  number = {11},
  pages = {999--1011},
  issn = {0167-8655},
  doi = {10.1016/S0167-8655(00)00059-3},
  urldate = {2025-10-05},
  abstract = {In this paper, we propose a robust motion estimation algorithm using uncalibrated 3D motion model considering depth discontinuity. Most of the previous direct motion estimation algorithms with 3D motion model compute the depth value through the local smoothing, which result in erroneous results at depth discontinuity. In this paper, we overcome this problem at depth discontinuity by adding discontinuity preserving regularization term to the original equation. Robust estimation enables motion segmentation through the dominant camera motion compensation. Experimental results show the improved result at the depth discontinuity.},
  keywords = {Direct method,Discontinuity,Optic flow,Uncalibrated}
}

@article{ha20013d,
  title = {{{3D}} Structure Recovery and Calibration under Varying Intrinsic Parameters Using Known Angles},
  author = {Ha, Jong-Eun and Kweon, In-So},
  year = {2001},
  month = feb,
  journal = {Pattern Recognition},
  volume = {34},
  number = {2},
  pages = {351--359},
  issn = {0031-3203},
  doi = {10.1016/S0031-3203(99)00229-0},
  urldate = {2025-10-05},
  abstract = {In this paper, we present a new algorithm for the calibration of a camera and the recovery of 3D scene structure up to a scale from image sequences using known angles between lines in the scene. The proposed method computes the intrinsic parameters of camera using the invariance of angles under the similarity transformation. Specifically, we recover the matrix that is the homography between the projective structure and the Euclidean structure using angles. Since this matrix is a unique one in the given set of image sequences, we can easily deal with the problem of varying intrinsic parameters of the camera. Experimental results on the synthetic and real images demonstrate the feasibility of the proposed algorithm.},
  keywords = {3D reconstruction,Angle,Calibration}
}

@article{ha2005initialization,
  title = {Initialization Method for Self-Calibration Using 2-Views},
  author = {Ha, Jong-Eun and Kang, Dong-Joong},
  year = {2005},
  month = jan,
  journal = {Pattern Recognition},
  volume = {38},
  number = {1},
  pages = {143--150},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2004.05.005},
  urldate = {2025-10-05},
  abstract = {Recently, 3D structure recovery through self-calibration of camera has been actively researched. Traditional calibration algorithm requires known 3D coordinates of the control points while self-calibration only requires the corresponding points of images, thus it has more flexibility in real application. In general, self-calibration algorithm results in the nonlinear optimization problem using constraints from the intrinsic parameters of the camera. Thus, it requires initial value for the nonlinear minimization. Traditional approaches get the initial values assuming they have the same intrinsic parameters while they are dealing with the situation where the intrinsic parameters of the camera may change. In this paper, we propose new initialization method using the minimum 2 images. Proposed method is based on the assumption that the least violation of the camera's intrinsic parameter gives more stable initial value. Synthetic and real experiment shows this result.},
  keywords = {3D recovery,Camera calibration,Initialization,Self-calibration}
}

@incollection{ha2006robust,
  title = {Robust {{Segmentation}} of {{Characters Marked}} on {{Surface}}},
  booktitle = {Intelligent {{Computing}} in {{Signal Processing}} and {{Pattern Recognition}}: {{International Conference}} on {{Intelligent Computing}}, {{ICIC}} 2006 {{Kunming}}, {{China}}, {{August}} 16--19, 2006},
  author = {Ha, Jong-Eun and Kang, Dong-Joong and Jeong, Mun-Ho and Lee, Wang-Heon},
  editor = {Huang, De-Shuang and Li, Kang and Irwin, George William},
  year = {2006},
  pages = {478--487},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-37258-5_49},
  urldate = {2025-10-05},
  abstract = {Optical character recognition (OCR) is widely used for automation. Typical OCR algorithm runs on each character and it include preprocessing step of separating each character from input image. Most segmentation algorithm runs well on good quality of image of machine printed. Also, barcode on surface would be good candidate. But, there are industrial applications that could not adopt barcode. In this case, identification code is marked directly on the surface of products. Characters produced by marking have height difference between character and background region. This makes it difficult to devise illumination system which guarantees good quality of image. New algorithm targeting robust segmentation of characters marked on surface is proposed. Proposed algorithm is based on consistent use of two profiles of accumulated magnitude of edge not only in finding of rectangular region containing identification code on input image but also in final segmentation. Final position of segmentation of each character is found by dynamic programming which guarantee global minimum. Feasibility of proposed algorithm is tested under various lighting condition.},
  isbn = {978-3-540-37258-5},
  langid = {english},
  keywords = {Background Region,Height Difference,Identification Code,Input Image,Rectangular Region}
}

@article{ha2007automatic,
  title = {Automatic Detection of Calibration Markers on a Chessboard},
  author = {Ha, Jong-Eun},
  year = {2007},
  month = oct,
  journal = {Optical Engineering},
  volume = {46},
  number = {10},
  pages = {107203},
  publisher = {SPIE},
  issn = {0091-3286, 1560-2303},
  doi = {10.1117/1.2789620},
  urldate = {2025-10-05},
  abstract = {The calibration of a camera is an essential step for the metric reconstruction of the world. Intrinsic and extrinsic parameters of the camera are computed using a one-to-one correspondence between 3-D and 2-D coordinates of control points on the calibration target. Three-dimensional coordinates of control points are given in advance, and 2-D coordinates are detected on the image. The calibration target uses cross lines, circles, and a chessboard pattern to improve detection of the control points on the image. We propose an algorithm to automatically detect the control points on an image, especially for a chessboard pattern. Two symmetric properties of a chessboard pattern related to geometric and brightness distribution are used, and two concentric circles are used as probes for the effective use of the two properties. The two symmetric properties of the chessboard generate the candidates of control points. Finally, a cross-ratio of four points on a line is used for verification. The experimental results using images that vary in scale, pose, and illumination demonstrate the robustness of the proposed algorithm.}
}

@article{ha2008new,
  title = {New Strain Measurement Method at Axial Tensile Test of Thin Films through Direct Imaging},
  author = {Ha, Jong-Eun and Park, Jun-Hyub and Kang, Dong-Joong},
  year = {2008},
  month = aug,
  journal = {Journal of Physics D: Applied Physics},
  volume = {41},
  number = {17},
  pages = {175406},
  issn = {0022-3727},
  doi = {10.1088/0022-3727/41/17/175406},
  urldate = {2025-10-05},
  abstract = {This paper proposes a new method for measuring strain during a tensile test of the specimen with micrometre size through direct imaging. A specimen was newly designed for adoption of direct imaging which was the main contribution of the proposed system. The structure of the specimen has eight indicators that make it possible to adopt direct imaging and it is fabricated using the same process of microelectromechanical system (MEMS) devices to guarantee the feasibility of the tensile test. We implemented a system for non-contact in situ measurement of strain with the specimen, the image-based displacement measurement system. Extension of the gauge length in the specimen could be found robustly by computing the positions of the eight rectangular-shape indicators on the image. Also, for an easy setup procedure, the region of interest was found automatically through the analysis of the edge projection profile along the horizontal direction. To gain confidence in the reliability of the system, the tensile test for the Al--3\%Ti thin film was performed, which is widely used as a material in MEMS devices. Tensile tests were performed and displacements were measured using the proposed method and also the capacitance type displacement sensor for comparison. It is demonstrated that the new strain measurement system can be effectively used in the tensile test of the specimen at microscale with easy setup and better accuracy.},
  langid = {english}
}

@article{ha2009automatic,
  title = {Automatic Detection of Chessboard and Its Applications},
  author = {Ha, Jong-Eun},
  year = {2009},
  month = jun,
  journal = {Optical Engineering},
  volume = {48},
  number = {6},
  pages = {067205},
  publisher = {SPIE},
  issn = {0091-3286, 1560-2303},
  doi = {10.1117/1.3156053},
  urldate = {2025-10-05},
  abstract = {A chessboard image can be used as a pattern for camera calibration and pose estimation. We propose an algorithm for the automatic detection of a chessboard using one circle as a probe. We first generate candidates using the Lucas-Kanade feature detector algorithm. Finally, we find control points on the chessboard using properties inherent on a chessboard pattern. We suggest that a distinct edge along the border line and black and white sequence on the chessboard can be used as constraints. A circle is used as a probe for checking those properties. We show that the proposed algorithm can be used successfully in camera calibration and pose estimation. Experimental results using images acquired under the variation of scale, pose, and illumination shows the robustness of proposed algorithm.}
}

@article{ha2010foreground,
  title = {Foreground Objects Detection Using Multiple Difference Images},
  author = {Ha, Jong-Eun and Lee, Wangheon},
  year = {2010},
  month = apr,
  journal = {Optical Engineering},
  volume = {49},
  number = {4},
  pages = {047201},
  publisher = {SPIE},
  issn = {0091-3286, 1560-2303},
  doi = {10.1117/1.3374043},
  urldate = {2025-10-05},
  abstract = {In visual surveillance, robust foreground object detection is an essential step for further processing such as segmentation, tracking, and extraction of a scene's contextual information. Typical approaches continuously update background images and use then for detecting foreground objects. They involve many parameters that should be adjusted according to the situation where surveillance cameras are operating. We propose an algorithm for the robust detection of foreground objects using multiple difference images that requires only one parameter to adjust. We show that the proposed algorithm gives comparable results with less computation time through experimental results using test images with groundtruths.}
}

@article{ha2011grouping,
  title = {Grouping Contents on {{Korean}} Road Signs},
  author = {Ha, Jong-Eun},
  year = {2011},
  month = dec,
  journal = {International Journal of Control, Automation and Systems},
  volume = {9},
  number = {6},
  pages = {1187--1193},
  issn = {2005-4092},
  doi = {10.1007/s12555-011-0620-6},
  urldate = {2025-10-04},
  abstract = {In this paper, we propose a method for grouping the content of Korean road signs into the five categories of Korean, English, direction symbol, road symbol and distance digit. The first step necessary to automate the inspection of road signs is to determine whether their content is reproduced as clearly as in the original design. Previously, inspections were performed manually by humans. Instead, we deal with images that are acquired manually by digital cameras, using various features including color, relative length and size, reflecting the design rule for Korean road sign. We began by analyzing blobs that are obtained through connected component analysis after binarization. These are first grouped into two categories: direction symbol and size. Then, the road symbol is selected using color. Finally, the remaining blobs are grouped using the relative length between blobs. Experimental results using real images show the feasibility of the proposed algorithm.},
  langid = {english},
  keywords = {Grouping,machine vision,road sign,segmentation}
}

@article{ha2011new,
  title = {A New Method for Detecting Data Matrix under Similarity Transform for Machine Vision Applications},
  author = {Ha, Jong-Eun},
  year = {2011},
  month = aug,
  journal = {International Journal of Control, Automation and Systems},
  volume = {9},
  number = {4},
  pages = {737--741},
  issn = {2005-4092},
  doi = {10.1007/s12555-011-0415-9},
  urldate = {2025-10-04},
  abstract = {Data matrices are widely used in the automotive, aerospace and computer manufacturing industries. In industry, they are used to identify objects used in process control. In this paper, we focus on detecting data matrices where a camera is configured to see it in a perpendicular direction that is typical in machine vision applications. In this case, the image projection can be modeled as a similarity transform. Data matrices are attached or marked by laser on the surface of objects, and have L-shaped solid lines which act as references for decoding. Under a similarity transform, distances from the center of a data matrix to each side of the L-shape are equal. This symmetric property is used to detect a data matrix, and experimental results show the feasibility of the proposed algorithm.},
  langid = {english},
  keywords = {2D barcode,Data matrix,machine vision,rotation angle}
}

@article{ha2011simple,
  title = {Simple Method for Calibrating Omnidirectional Stereo with Multiple Cameras},
  author = {Ha, Jong-Eun and Choi, I.-Sak},
  year = {2011},
  month = apr,
  journal = {Optical Engineering},
  volume = {50},
  number = {4},
  pages = {043608},
  publisher = {SPIE},
  issn = {0091-3286, 1560-2303},
  doi = {10.1117/1.3567070},
  urldate = {2025-10-04},
  abstract = {Cameras can give useful information for the autonomous navigation of a mobile robot. Typically, one or two cameras are used for this task. Recently, an omnidirectional stereo vision system that can cover the whole surrounding environment of a mobile robot is adopted. They usually adopt a mirror that cannot offer uniform spatial resolution. In this paper, we deal with an omnidirectional stereo system which consists of eight cameras where each two vertical cameras constitute one stereo system. Camera calibration is the first necessary step to obtain 3D information. Calibration using a planar pattern requires many images acquired under different poses so it is a tedious step to calibrate all eight cameras. In this paper, we present a simple calibration procedure using a cubic-type calibration structure that surrounds the omnidirectional stereo system. We can calibrate all the cameras on an omnidirectional stereo system in just one shot.}
}

@article{ha2012extrinsic,
  title = {Extrinsic Calibration of a Camera and Laser Range Finder Using a New Calibration Structure of a Plane with a Triangular Hole},
  author = {Ha, Jong-Eun},
  year = {2012},
  month = dec,
  journal = {International Journal of Control, Automation and Systems},
  volume = {10},
  number = {6},
  pages = {1240--1244},
  issn = {2005-4092},
  doi = {10.1007/s12555-012-0619-7},
  urldate = {2025-10-04},
  abstract = {Sensor fusion of a camera and laser range finder is important for the autonomous navigation of mobile robots. Finding the transformation between the camera and laser range finder is the first necessary step for the fusion of information. Many algorithms have been proposed, but these tend to require many different steps in order to achieve reliable and accurate results. A calibration structure that has triangular hole on its plane is proposed for the extrinsic calibration of a camera and laser range finder. Locations of laser scan data that are invisible on the calibration plane can be determined using property on the proposed calibration structure. First, we classify the laser scan data into two groups where one is on the plane and the other is off the plane. Then, we determine the absolute location of the laser scan data on the plane through a search of the parameters of the line. Finally, we can establish 3D-3D correspondences between the camera and laser range finder. Extrinsic calibration between a camera and laser range finder is found using a conventional 3D-3D transformation computing algorithm. Keywords: Calibration k]camera k]extrinsic calibration k]laser range finder},
  langid = {english},
  keywords = {Autonomous Navigation,Camera Calibration,Extrinsic Parameter,Linear Solution,Mobile Robot}
}

@article{ha2013calibration,
  title = {Calibration of Structured Light Stripe System Using Plane with Slits},
  author = {Ha, Jong-Eun and Her, Kang-Wook},
  year = {2013},
  month = jan,
  journal = {Optical Engineering},
  volume = {52},
  number = {1},
  pages = {013602},
  publisher = {SPIE},
  issn = {0091-3286, 1560-2303},
  doi = {10.1117/1.OE.52.1.013602},
  urldate = {2025-10-04},
  abstract = {Structured light stripe systems are widely used in industrial applications for acquiring three-dimensional (3-D) information. Calibration is the first necessary step and traditional algorithms compute the 4{\texttimes}3 transformation matrix from image to world. Therefore, 3-D information can be obtained with respect to the camera's coordinate system, and it cannot be transformed into the laser coordinate system. We propose a new calibration algorithm of a structured light system that can compute the relative pose of the laser coordinate system with respect to the camera coordinate system. We can convert 3-D information either to the laser coordinate system or to the camera coordinate system. The proposed algorithm uses two planes where one plane with multiple slits is set perpendicular to the other plane. We can easily compute the position and rotation of the laser coordinate system using proposed calibration structure. Also, we can solve the extrinsic calibration using just one shot of an image while conventional algorithms require more than two images under different poses. Experiments under various configurations show the feasibility of proposed algorithm.}
}

@article{ha2013image,
  title = {An Image Processing Algorithm for the Automatic Manipulation of Tie Rod},
  author = {Ha, Jong-Eun},
  year = {2013},
  month = oct,
  journal = {International Journal of Control, Automation and Systems},
  volume = {11},
  number = {5},
  pages = {984--990},
  issn = {2005-4092},
  doi = {10.1007/s12555-012-0545-8},
  urldate = {2025-10-04},
  abstract = {In this paper, we present a robust image processing algorithm for the automatic manipulation of tie rod in the automotive assembly process. A sensing system consisting of a camera and a slit laser is used to obtain 3D data for processing. It is attached to the robot's arm. A nut runner is used as an end-effector of the robot to manipulate the bolt and nut on the tie rod. For the successful operation, a robust image processing algorithm that can cope with diverse illumination variations in manufacturing conditions is required. We present a robust algorithm for the extraction of laser contours on images. It consists of two steps where color and grey information is used to cope with clutters caused by laser reflections on metal surfaces. Experimental results using many images under real conditions show the robustness of the proposed algorithm.},
  langid = {english},
  keywords = {3D sensing,automatic assembly,image processing,laser contour}
}

@article{ha2014automatic,
  title = {{Automatic Manipulation of Tie Rod using Robot with 3D Sensing System}},
  author = {Ha, Jong-Eun and Lee, Wang-Heon},
  year = {2014},
  month = nov,
  journal = {Journal of Electrical Engineering \& Technology},
  volume = {9},
  number = {6},
  pages = {2162--2167},
  issn = {1975-0102},
  urldate = {2025-10-04},
  abstract = {Robots are widely used in various automation processes in industrial applications. Traditionally, it operated under fixed condition by teaching operating positions. Recently, diverse 2D/3D sensors are used together with robot to give more flexibility in operation. In this paper, we deal with automatic manipulation of tie rod in automotive production line. Sensor system consisted of a camera and slit laser is used for the acquisition of 3D information and it is used attached on the robot. Nut runner is used for the manipulation of stop nut and adjust bolt on the tie rod. Detailed procedures for the automatic manipulation of tie rod are presented. In the presented approach, we effectively use 3D information in whole procedure such as computing distance to the tie rod, rotation angle of bolt and nut. Experimental results show the feasibility of the proposed algorithm.},
  langid = {korean},
  keywords = {3D sensing,Machine vision,Robot manipulation,Robot vision}
}

@article{ha2015automatic,
  title = {{Automatic Registration of Two Parts using Robot with Multiple 3D Sensor Systems}},
  author = {Ha, Jong-Eun},
  year = {2015},
  month = jul,
  journal = {Journal of Electrical Engineering \& Technology},
  volume = {10},
  number = {4},
  pages = {1830--1835},
  issn = {1975-0102},
  urldate = {2025-10-04},
  abstract = {In this paper, we propose an algorithm for the automatic registration of two rigid parts using multiple 3D sensor systems on a robot. Four sets of structured laser stripe system consisted of a camera and a visible laser stripe is used for the acquisition of 3D information. Detailed procedures including extrinsic calibration among four 3D sensor systems and hand/eye calibration of 3D sensing system on robot arm are presented. We find a best pose using search-based pose estimation algorithm where cost function is proposed by reflecting geometric constraints between sensor systems and target objects. A pose with minimum gap and height difference is found by greedy search. Experimental result using demo system shows the robustness and feasibility of the proposed algorithm.},
  langid = {korean}
}

@article{ha2015improved,
  title = {Improved Algorithm for the Extrinsic Calibration of a Camera and Laser Range Finder Using {{3D-3D}} Correspondences},
  author = {Ha, Jong-Eun},
  year = {2015},
  month = oct,
  journal = {International Journal of Control, Automation and Systems},
  volume = {13},
  number = {5},
  pages = {1272--1276},
  issn = {2005-4092},
  doi = {10.1007/s12555-013-0528-4},
  urldate = {2025-10-04},
  abstract = {Extrinsic calibration between a camera and laser range finder (LRF) is required to integrate information from both sensor. In this paper, we propose an improved algorithm of our earlier work [1] for the extrinsic calibration of a camera and laser range finder. We improve our previous algorithm in two directions. First, control points on laser range finder's data are obtained by finding the crossing point of two fitted lines where previous algorithm directly used laser range finder's data. It can reduce the effect of noisy measurement of laser range finder. Second, difference of distance on calibration plane is used as cost where previous algorithm used matching portion. Proposed algorithm gives an improved result than previous one. It is verified thorough experimental results.},
  langid = {english},
  keywords = {Calibration,camera,extrinsic calibration,laser range finder}
}

@article{ha2018calibration,
  title = {{Calibration of Structured Light Vision System using Multiple Vertical Planes}},
  author = {Ha, Jong Eun},
  year = {2018},
  month = jan,
  journal = {Journal of Electrical Engineering \& Technology},
  volume = {13},
  number = {1},
  pages = {438--444},
  issn = {1975-0102},
  urldate = {2025-10-04},
  abstract = {Structured light vision system has been widely used in 3D surface profiling. Usually, it is composed of a camera and a laser which projects a line on the target. Calibration is necessary to acquire 3D information using structured light stripe vision system. Conventional calibration algorithms have found the pose of the camera and the equation of the stripe plane of the laser under the same coordinate system of the camera. Therefore, the 3D reconstruction is only possible under the camera frame. In most cases, this is sufficient to fulfill given tasks. However, they require multiple images which are acquired under different poses for calibration. In this paper, we propose a calibration algorithm that could work by using just one shot. Also, proposed algorithm could give 3D reconstruction under both the camera and laser frame. This would be done by using newly designed calibration structure which has multiple vertical planes on the ground plane. The ability to have 3D reconstruction under both the camera and laser frame would give more flexibility for its applications. Also, proposed algorithm gives an improvement in the accuracy of 3D reconstruction.},
  langid = {korean},
  keywords = {3D reconstruction,Calibration,Extrinsic calibration,Structured light vision system}
}

@article{hajongeun1996pyomyeonyi,
  title = {표면의 반사모델을 이용한 타이어 정보마크의 추출},
  author = {{하종은} and {이재용} and {권인소}},
  year = {1996},
  journal = {제어자동화시스템 공학 논문지},
  volume = {2},
  number = {4},
  pages = {324--329}
}

@article{hajongeun2004crt,
  title = {{CRT 판넬의 첵 불량 검출을 위한 새로운 조명 시스템}},
  author = {하종은 and 차준혁 and 권인소},
  year = {2004},
  month = jun,
  journal = {제어로봇시스템학회 논문지},
  volume = {10},
  number = {6},
  pages = {487--493},
  issn = {1976-5622},
  urldate = {2025-10-05},
  abstract = {하종은, 차준혁, 권인소 {\textbar} 제어로봇시스템학회 논문지 {\textbar} 2004.6},
  langid = {korean}
}

@article{hajongeun2004gagdo,
  title = {{각도 정보를 이용한 카메라 보정 알고리듬}},
  author = {하종은 and 권인소},
  year = {2004},
  month = may,
  journal = {제어로봇시스템학회 논문지},
  volume = {10},
  number = {5},
  pages = {415--420},
  issn = {1976-5622},
  urldate = {2025-10-05},
  abstract = {하종은, 권인소 {\textbar} 제어로봇시스템학회 논문지 {\textbar} 2004.5},
  langid = {korean}
}

@article{hajongeun2016gyeonggye,
  title = {{경계 추출 및 처리를 통한 다이아몬드 휠 검사}},
  author = {하종은},
  year = {2016},
  month = nov,
  journal = {제어로봇시스템학회 논문지},
  volume = {22},
  number = {11},
  pages = {932--936},
  issn = {1976-5622},
  doi = {10.5302/J.ICROS.2016.16.0147},
  urldate = {2025-10-04},
  abstract = {In this paper, we present a method for the inspection of diamond wheels. In total, six items, including height, radius, and angle, need to be checked during the manufacturing of a diamond wheel. Automatic inspection through image processing is presented in this paper. First, a contour corresponding to the boundary of the diamond wheel is extracted from an image. Next, control points are selected by processing the contour. Seven control points are detected and used for the computation of the required item. Detailed procedures for the computation of the height, radius, and angle using control points are presented in this paper. Experimental results show the feasibility of the presented method.},
  langid = {korean},
  keywords = {diamond wheel,inspection,machine vision}
}

@article{hajongeun2016kalra,
  title = {{칼라 나사 검사를 위한 표면 영역 자동 검출}},
  author = {하종은 and 송태훈},
  year = {2016},
  month = jan,
  journal = {한국전자통신학회 논문지},
  volume = {11},
  number = {1},
  pages = {107--112},
  issn = {1975-8170},
  urldate = {2025-10-04},
  abstract = {나사는 산업의 모든 분야에서 널리 사용되는 중요한 부품이다. 최근에는 여러 가지 필요에 의해 다양한 칼라 나사가 출시되고 있다. 이에 따라 제조 공정상에서 실시간 품질 검사가 요구되고 있다. 본 논문에서는 칼라 정보와 동적 계획법(dynamic programming) 알고리듬을 이용한 칼라 나사 검사를 위한 표면 영역 자동 추출 알고리듬에 대해 다루도록 한다. 나사의 외곽 경계는 칼라 성분의 차이를 이용하여 보다 강인한 검출이 가능하도록 한다. 나사의 내부 경계는 직교 좌표계를 극좌표계로 변환후 흑백 이미지상에서 일정 영역의 밝기값 차이를 이용한 동적 계획법을 적용하여 추출하도록 한다. 실험에서는 동일한 인자값을 이용한 결과를 분석하도록 한다.},
  langid = {korean},
  keywords = {,Color,Fastener,Fastener Inspection,Machine Vision}
}

@article{hajongeun2016makeowa,
  title = {{마커와 카메라를 이용한 스키드 구동 이동 로봇의 회전 운동 분석}},
  author = {하종은},
  year = {2016},
  month = jan,
  journal = {한국전자통신학회 논문지},
  volume = {11},
  number = {2},
  pages = {185--190},
  issn = {1975-8170},
  urldate = {2025-10-04},
  abstract = {본 논문에서는 이동 로봇에 마커를 부착하여 이를 카메라를 통해 자동으로 추출하여 이동 로봇의 운동 특성을 분석하는 방법에 대해 다루도록 한다. 이동 로봇의 구동을 위한 제어 알고리듬의 개발이나 자율 이동 관련 알고리듬 개발시 인자값에 따른 운동 특성 분석은 중요한 부분이다. 이의 분석을 위해 본 논문에서는 네 개의 체스보드 형태의 마커를 이동 로봇에 부착하여 사용하도록 한다. 이들 네 개의 마커들은 동일 평면에 존재하도록 배치하도록 한다. 평면 호모그라피를 이용하여 로봇의 실제 이동량을 계산하도록 한다. 제시된 방법은 P3-AT 로봇을 이용하여 실험을 수행하였으며 안정적인 운동 분석이 가능하였다.},
  langid = {korean},
  keywords = {,Mobile Robot,Motion Monitoring,Plane Homography,Robot Vision}
}

@article{hajongeun2017ijinhwa,
  title = {{이진화 및 블랍 처리를 이용한 코일 돌출 검사}},
  author = {하종은},
  year = {2017},
  month = jun,
  journal = {제어로봇시스템학회 논문지},
  volume = {23},
  number = {6},
  pages = {469--473},
  issn = {1976-5622},
  doi = {10.5302/J.ICROS.2017.17.0030},
  urldate = {2025-10-04},
  abstract = {In this paper we present a method for the inspection of protrusion of coils along the circumference of a given circle. Perspective distortion makes a circle in a world an ellipse on image. This makes protrusion inspection difficult. Our approach makes this problem simple by adjusting a camera's image plane to be parallel to the object's plane. Illumination system uses a back light and coaxial light which enables a robust and simple detection of a reference point for the inspection. First, a center of the circle is found through binarization and blob processing. Inspection area along the circumference of the circle is converted into rectangular image type using polar coordinates. Finally, binarization and blob processing is applied to detect the intrusion of the coil. Experimental results show the feasibility of the presented algorithm.},
  langid = {korean},
  keywords = {blob processing,coil,machine vision,protrusion inspection}
}

@article{hajongeun2018sayeong,
  title = {{사영 변환하의 데이터 매트릭스 검출}},
  author = {하종은},
  year = {2018},
  month = dec,
  journal = {제어로봇시스템학회 논문지},
  volume = {24},
  number = {12},
  pages = {1117--1122},
  issn = {1976-5622},
  doi = {10.5302/J.ICROS.2018.18.0171},
  urldate = {2025-10-04},
  abstract = {Data Matrix is used in various industrial fields to embed diverse information in a compact fashion. Data Matrix is usually attached by printing method or is marked by laser on the surface of objects. In addition, cameras are used for decoding. It has an Lshape comprised of two perpendicular solid lines that serve to indicate the correct orientation of the code and the boundaries of the data area. Conventional decoding algorithms of Data Matrix require an image acquired under similarity transform. We present an algorithm for the robust detection of Data Matrix under general perspective transform. The presented algorithm first detects the whole area containing the Data Matrix using image binarization, connected component analysis and morphology. Next, corner points corresponding to the L-shape in the Data Matrix are detected using line fitting through polygonal approximation after contour processing. Finally, the Data Matrix is converted into canonical image using homography that is computed using the four detected corner points of the Data Matrix. Experiments using images having large perspective distortions acquired under various pose demonstrates the robustness of presented method.},
  langid = {korean},
  keywords = {2D barcode,data matrix,machine vision,pattern recognition}
}

@article{hajongeun2019sayeong,
  title = {{사영 변환하의 합성 이미지를 이용한 Mask R-CNN 기반 QR 코드 검출}},
  author = {하종은},
  year = {2019},
  month = sep,
  journal = {제어로봇시스템학회 논문지},
  volume = {25},
  number = {9},
  pages = {770--774},
  issn = {1976-5622},
  doi = {10.5302/J.ICROS.2019.19.0117},
  urldate = {2025-10-04},
  abstract = {Various types of 2D barcodes including QR code, Data Matrix are widely used in diverse industries. Recently, QR code is adopted in mobile phone. They require QR code occupy some amount of area on the image to correctly operate. In other application of mobile robot navigation where QR code is used as landmarks, we need to detect QR code in various scale and lighting condition. Traditional approaches operate well under restricted conditions also they require the setting of many parameters. In this paper, we deal with the detection of QR code in wild including various lighting condition and scale. We adopt the Mask R-CNN [11] for the detection of QR code. It requires many training images to have good performance. We present a method that uses composite images which is made under general perspective transform. QR code under reference poses and real images are composited by the presented method. We present various experimental results under diverse configuration of hyperparameters. Sequential step of first training using many composite images then finally training using real images show the best performance. Experimental results show the feasibility of presented approach.},
  langid = {korean},
  keywords = {2D barcode,convolutional neural networks,deep learning,QR code,transfer learning}
}

@article{hajongeun2020habseong,
  title = {{합성 이미지를 이용한 Mask R-CNN 기반 한국 번호판 검출}},
  author = {하종은},
  year = {2020},
  month = sep,
  journal = {제어로봇시스템학회 논문지},
  volume = {26},
  number = {9},
  pages = {778--783},
  issn = {1976-5622},
  doi = {10.5302/J.ICROS.2020.20.0070},
  urldate = {2025-10-04},
  abstract = {License plate detection on an image is the first necessary step for the automatic recognition of license plate. In this paper, we adopt Mask R-CNN [11] to detect a license plate on an image. It requires many training images to cope with over-fitting that occurs when training samples are smaller than numbers of parameters. In general, more than 200K images are required for the stable training, but it requires large amount of time and cost. In this paper, we present a method that uses already available open dataset. We use two open dataset of CCPD [8] and BDD [18]. CCDP dataset provides locations of four corner points on an image. But, they contain license plate of China. First, Korean license plate image is made by referencing the design rule. Then, Korea license plate image is projected onto the corresponding positions of CCPD image. Four corresponding points between Korea license plate and CCPD images is used in the computation of perspective transform. Two different types of training images from CCPD and BDD dataset are used in the training of Mask R-CNN, and they are applied to image that contains real Korea license plate. Training using composite images from CCPD shows better performance than that of BDD on the real Korea license plate image. Experimental results show the feasibility of presented approach.},
  langid = {korean},
  keywords = {convolutional neural networks,deep learning,license plate,license plate detection}
}

@article{heogeon2015dongjaginsigeul,
  title = {{동작인식을 이용한 탁구 스윙 분석}},
  author = {허건 and 하종은},
  year = {2015},
  month = jan,
  journal = {제어로봇시스템학회 논문지},
  volume = {21},
  number = {1},
  pages = {40--45},
  issn = {1976-5622},
  urldate = {2025-10-04},
  abstract = {In this paper, we present an algorithm for the analysis of poses while playing table-tennis using action recognition. We use Kinect as the 3D sensor and 3D skeleton data provided by Kinect for further processing. We adopt a spherical coordinate system and feature selected using k-means clustering. We automatically detect the starting and ending frame and discriminate the action of tabletennis into two groups of forehand and backhand swing. Each swing is modeled using HMM(Hidden Markov Model) and we used a dataset composed of 200 sequences from two players. We can discriminate two types of table tennis swing in real-time. Also, it can provide analysis according to similarities found in good poses.},
  langid = {korean},
  keywords = {action recognition,HMM (Hidden Markov Model),k-means clustering,Kinect}
}

@article{janggyeongseog2018kalra,
  title = {{칼라 정보를 이용한 스키 기문 검출}},
  author = {장경석 and 하종은},
  year = {2018},
  month = dec,
  journal = {제어로봇시스템학회 논문지},
  volume = {24},
  number = {12},
  pages = {1123--1127},
  issn = {1976-5622},
  doi = {10.5302/J.ICROS.2018.18.0177},
  urldate = {2025-10-04},
  abstract = {Autonomous ski robots require robust detection of ski gates when using sensors. Various sensors including 3D LiDAR, 2D LiDAR, stereo systems and cameras might have the potential for detecting ski gates. In this paper, we consider a camera as the main sensor for detecting ski gates by considering processing time and cost. However, cameras have difficulties in coping with variations due to illumination variations. We present a robust algorithm for detecting ski gate using color information. Color differences are used for detecting red and blue ski gates. Binarization using color differences is first performed then morphological erosion and dilation is applied to remove small background objects. Finally, a connected component analysis is performed to find the ski gates. Experimental results using images acquired from real ski slopes demonstrate the feasibility of the presented algorithm.},
  langid = {korean},
  keywords = {autonomous navigation,robot vision,ski gates detection,skiing robot}
}

@article{janggyeongseog2019kalra,
  title = {{칼라 체스보드를 이용한 RGB-D 센서와 로봇의 외부보정}},
  author = {장경석 and 하종은},
  year = {2019},
  month = jan,
  journal = {제어로봇시스템학회 논문지},
  volume = {25},
  number = {1},
  pages = {63--68},
  issn = {1976-5622},
  doi = {10.5302/J.ICROS.2019.18.0172},
  urldate = {2025-10-04},
  abstract = {In this paper, we present a method for the extrinsic calibration of a robot and an RGB-D sensor using a color chessboard. The color pattern of red, green, and blue is used on a conventional gray chessboard to identify the coordinate frame under any pose of the chessboard. The chessboard is attached on the tool center point (TCP) of the robot, and the cross points of the chessboard are automatically detected for the extrinsic calibration. We compared the proposed method with the conventional method that uses a chessboard on the ground plane. The accuracy of extrinsic calibration was evaluated by checking the final location of a tip at the TCP of the robot. The proposed method exhibited improved accuracy for the entire working volume as compared to the conventional method.},
  langid = {korean},
  keywords = {color chessboard,extrinsic calibration,object manipulation,RGB-D sensor,robot}
}

@article{kang2003fast,
  title = {Fast Object Recognition Using Dynamic Programming from Combination of Salient Line Groups},
  author = {Kang, Dong Joong and Ha, Jong Eun and Kweon, In So},
  year = {2003},
  month = jan,
  journal = {Pattern Recognition},
  volume = {36},
  number = {1},
  pages = {79--90},
  issn = {0031-3203},
  doi = {10.1016/S0031-3203(02)00046-8},
  urldate = {2025-10-05},
  abstract = {This paper presents a new method of grouping and matching line segments to recognize objects. We propose a dynamic programming-based formulation extracting salient line patterns by defining a robust and stable geometric representation that is based on perceptual organizations. As the endpoint proximity, we detect several junctions from image lines. We then search for junction groups by using the collinear constraint between the junctions. Junction groups similar to the model are searched in the scene, based on a local comparison. A DP-based search algorithm reduces the time complexity for the search of the model lines in the scene. The system is able to find reasonable line groups in a short time.},
  keywords = {Dynamic programming,Feature matching,Object recognition,Perceptual grouping}
}

@incollection{kang2006method,
  title = {A {{Method}} for {{Camera Pose Estimation}} from {{Object}} of a {{Known Shape}}},
  booktitle = {Intelligent {{Computing}} in {{Signal Processing}} and {{Pattern Recognition}}: {{International Conference}} on {{Intelligent Computing}}, {{ICIC}} 2006 {{Kunming}}, {{China}}, {{August}} 16--19, 2006},
  author = {Kang, Dong-Joong and Ha, Jong-Eun and Jeong, Mun-Ho},
  editor = {Huang, De-Shuang and Li, Kang and Irwin, George William},
  year = {2006},
  pages = {606--613},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-37258-5_64},
  urldate = {2025-10-05},
  abstract = {Pose estimation between cameras and object is a central element for computer vision and its applications. In this paper, we present an approach to solve the problem of estimating the camera 3-D location and orientation from a matched set of 3-D model and 2-D image features. We derive an error equation using roll-pitch-yaw angle to present the rotation matrix and directly calculate the partial derivatives of Jacobian matrix without use of numerical methods for estimation parameters from the nonlinear error equation. Because the proposed method does not use a numerical method to derive the partial derivatives, it is very fast and so adequate for real-time pose estimation and also insensitive to selection of initial values for solving the nonlinear equation. The method is proved from real image experiments and a comparison with a numerical estimation method is presented.},
  isbn = {978-3-540-37258-5},
  langid = {english},
  keywords = {Camera Calibration,Error Equation,Image Line,Model Line,Projection Matrix}
}

@article{kang2008detection,
  title = {{Detection of Calibration Patterns for Camera Calibration with Irregular Lighting and Complicated Backgrounds}},
  author = {Kang, Dong-Joong and Ha, Jong-Eun and Jeong, Mun-Ho},
  year = {2008},
  month = oct,
  journal = {International Journal of Control Automation and Systems},
  volume = {6},
  number = {5},
  pages = {746--754},
  issn = {1598-6446},
  urldate = {2025-10-05},
  abstract = {This paper proposes a method to detect calibration patterns for accurate camera calibration under complicated backgrounds and uneven lighting conditions of industrial fields. Required to measure object dimensions, the preprocessing of camera calibration must be able to extract calibration points from a calibration pattern. However, industrial fields for visual inspection rarely provide the proper lighting conditions for camera calibration of a measurement system. In this paper, a probabilistic criterion is proposed to detect a local set of calibration points, which would guide the extraction of other calibration points in a cluttered background under irregular lighting conditions. If only a local part of the calibration pattern can be seen, input data can be extracted for camera calibration. In an experiment using real images, we verified that the method can be applied to camera calibration for poor quality images obtained under uneven illumination and cluttered background.},
  langid = {korean},
  keywords = {Camera calibration,detection of calibration pattern,length measurement system,machine vision,uneven illumination}
}

@article{kang2009detection,
  title = {A Detection Cell Using Multiple Points of a Rotating Triangle to Find Local Planar Regions from Stereo Depth Data},
  author = {Kang, Dong-Joong and Lim, Sung-Jo and Ha, Jong-Eun and Jeong, Mun-Ho},
  year = {2009},
  month = apr,
  journal = {Pattern Recognition Letters},
  volume = {30},
  number = {5},
  pages = {486--493},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2008.11.011},
  urldate = {2025-10-05},
  abstract = {This paper presents a method to recognize plane regions for unobstructed motion of mobile robots. When an autonomous agency, using a stereo camera or a laser scanning sensor, is in an unknown 3D environment, the mobile agency must detect the plane regions so that it can independently decide its direction of movement in order to perform assigned tasks. In this paper, a fast method of plane detection is proposed, wherein the normal vector of a triangle is inscribed in a small circular region such that the normal vector passes through the circumcenter area of the triangle. To reduce the effects of noise and outliers, the triangle is rotationally sampled with respect to the center position of the circular region, and a series of inscribed triangles having different normal vectors is generated. The direction vectors of these generated triangles are normalized and the median direction of the normal vectors is then used to test the planarity of the circular region. A pose finding procedure is introduced from range data of a surface to decide the scale and rotation angle of the circular region superimposed on range image data. The method of plane detection is very fast as computation of local information about the plane typically requires sub-ms duration, and the performance of the algorithm for real range data obtained from a stereo camera system has been verified.},
  keywords = {Mobile robot,Obstacle detection,Plane recognition,Range data,Stereo sensor}
}

@article{kang2010specimen,
  title = {Specimen Alignment in an Axial Tensile Test of Thin Films Using Direct Imaging and Its Influence on the Mechanical Properties of {{BeCu}}},
  author = {Kang, Dong-Joong and Park, Jun-Hyub and Shin, Myung-Soo and Ha, Jong-Eun and Lee, Hak-Joo},
  year = {2010},
  month = jun,
  journal = {Journal of Micromechanics and Microengineering},
  volume = {20},
  number = {8},
  pages = {085001},
  issn = {0960-1317},
  doi = {10.1088/0960-1317/20/8/085001},
  urldate = {2025-10-05},
  abstract = {This paper proposes a new system for verification of the alignment of loading fixtures and test specimens during tensile testing of thin film with a micrometer size through direct imaging. The novel and reliable image recognition system to evaluate the misalignment between the load train and the specimen axes during tensile test of thin film was developed using digital image processing technology with CCD. The decision of whether alignment of the tensile specimen is acceptable or not is based on a probabilistic analysis through the edge feature extraction of digital imaging. In order to verify the performance of the proposed system and investigate the effect of the misalignment of the specimen on tensile properties, the tensile tests were performed as displacement control in air and at room temperature for metal thin film, the beryllium copper (BeCu) alloys. In the case of the metal thin films, bending stresses caused by misalignment are insignificant because the films are easily bent during tensile tests to eliminate the bending stresses. And it was observed that little effects and scatters on tensile properties occur by stress gradient caused by twisting at in-plane misalignment, and the effects and scatters on tensile properties are insignificant at out-of-plane misalignment, in the case of the BeCu thin film.},
  langid = {english}
}

@article{kim2019one,
  title = {One {{Shot Extrinsic Calibration}} of a {{Camera}} and {{Laser Range Finder Using Vertical Planes}}},
  author = {Kim, Nam-Hun and Ha, Jong-Eun},
  year = {2019},
  month = mar,
  journal = {Journal of Electrical Engineering \& Technology},
  volume = {14},
  number = {2},
  pages = {917--922},
  issn = {2093-7423},
  doi = {10.1007/s42835-019-00087-z},
  urldate = {2025-10-04},
  abstract = {In this paper, we present a new algorithm for the extrinsic calibration of a camera and laser range finder just using one-shot data. Most previous algorithms require multiple sets of data acquired under different poses. We newly designed a calibration structure where multiple polyhedrons are set in predefined positions on the ground plane which allow the natural application of multiple point-line constraints for the extrinsic calibration of a camera and laser range finder. Control points are extracted from laser range finder's data by hand. If automatic detection of control points is possible, presented algorithm can be used for in-situ extrinsic calibration of a camera and laser range finder. Even using one-shot of data, presented algorithm gives comparable accuracy compared to conventional algorithm which requires multiple sets of data under different pose.},
  langid = {english},
  keywords = {Calibration,Camera,Extrinsic calibration,Laser range finder}
}

@article{kim2020extrinsic,
  title = {Extrinsic {{Calibration}} of a {{Camera}} and a {{2D LiDAR Using}} a {{Dummy Camera With IR Cut Filter Removed}}},
  author = {Kim, Jae-Yeul and Ha, Jong-Eun},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {183071--183079},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.3029267},
  urldate = {2025-10-04},
  abstract = {Extrinsic calibration of a camera and a LiDAR is necessary to fuse information from each sensor. The real trajectory of the LiDAR is not visible on an image, therefore the accuracy of the extrinsic calibration is usually checked by evaluating residuals of constraints. In this paper, we present an improved extrinsic calibration algorithm of a camera and a 2D LiDAR using an additional dummy camera removing IR cut filter, which make it possible to observe the real trajectory of LiDAR. Some previous algorithms used the real trajectory of LiDAR for the extrinsic calibration. However, they used IR filter directly on the calibrating camera by adjusting exposure time, which can affect the result of the extrinsic calibration. We use an initial solution using the Hu algorithm which makes extrinsic calibration possible by using just one shot of data. The Hu algorithm gives a sensitive result according to pose variation between a system consisted of a camera and a LiDAR and a calibration structure, which is verified using the real trajectory of LiDAR. We cope with this problem by refining the initial solution through nonlinear minimization in a 3D space using the real trajectory of LiDAR. Experimental results show that the proposed algorithm gives an improved solution.},
  keywords = {Calibration,camera,Cameras,Extrinsic calibration,Filtering algorithms,IR cut filter,Laser radar,LiDAR,sensor fusion,Three-dimensional displays,Trajectory,Two dimensional displays}
}

@article{kim2020foreground,
  title = {Foreground {{Objects Detection Using}} a {{Fully Convolutional Network With}} a {{Background Model Image}} and {{Multiple Original Images}}},
  author = {Kim, Jae-Yeul and Ha, Jong-Eun},
  year = {2020},
  journal = {IEEE Access},
  volume = {8},
  pages = {159864--159878},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.3020818},
  urldate = {2025-10-04},
  abstract = {Visual surveillance aims to reliably extract foreground objects. Traditional algorithms usually use a background model image which is generated through the probabilistic modeling of changes over time and space. They detect foreground objects by comparing a background model image with a current image. Hard shadows, illumination changes, camouflage, camera jitter, and ghost object motion make the robust detection of foreground objects difficult in visual surveillance. Recently, various methods based on deep learning have been applied to visual surveillance. It has been shown that deep learning approaches can stably extract salient features, and they give a superior result compared to traditional algorithms. However, they show a good performance only for scenes that are similar to a scene used in training. Without retraining on a new scene, they give a worse result compared to traditional algorithms. In this paper, we propose a stable foreground object detection algorithm through the integration of a background model image used in traditional methods and deep learning methods. A background model image generated by SuBSENSE and multiple images are used as the input of a fully convolutional network. Also, it is shown that it is possible to improve a generalization power by training the proposed network using diverse scenes from an open dataset. We show that the proposed algorithm can have a superior result compared to deep learning-based and traditional algorithms in a new scene without retraining the network. The performance of the proposed algorithm is evaluated using various datasets such as the CDnet 2014, SBI, LASIESTA, and our own datasets. The proposed algorithm shows improvement of 17.5\%, 8.9\%, and 4.3\%, respectively, in FM score compared to three deep learning-based algorithms.},
  keywords = {Classification algorithms,deep learning,Foreground objects detection,fully convolutional network,generalization power,Image segmentation,Machine learning,Object detection,Surveillance,Training,visual surveillance,Visualization}
}

@article{kim2021foreground,
  title = {Foreground {{Objects Detection}} by {{U-Net}} with {{Multiple Difference Images}}},
  author = {Kim, Jae-Yeul and Ha, Jong-Eun},
  year = {2021},
  month = jan,
  journal = {Applied Sciences},
  volume = {11},
  number = {4},
  pages = {1807},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2076-3417},
  doi = {10.3390/app11041807},
  urldate = {2025-10-03},
  abstract = {In video surveillance, robust detection of foreground objects is usually done by subtracting a background model from the current image. Most traditional approaches use a statistical method to model the background image. Recently, deep learning has also been widely used to detect foreground objects in video surveillance. It shows dramatic improvement compared to the traditional approaches. It is trained through supervised learning, which requires training samples with pixel-level assignment. It requires a huge amount of time and is high cost, while traditional algorithms operate unsupervised and do not require training samples. Additionally, deep learning-based algorithms lack generalization power. They operate well on scenes that are similar to the training conditions, but they do not operate well on scenes that deviate from the training conditions. In this paper, we present a new method to detect foreground objects in video surveillance using multiple difference images as the input of convolutional neural networks, which guarantees improved generalization power compared to current deep learning-based methods. First, we adjust U-Net to use multiple difference images as input. Second, we show that training using all scenes in the CDnet 2014 dataset can improve the generalization power. Hyper-parameters such as the number of difference images and the interval between images in difference image computation are chosen by analyzing experimental results. We demonstrate that the proposed algorithm achieves improved performance in scenes that are not used in training compared to state-of-the-art deep learning and traditional unsupervised algorithms. Diverse experiments using various open datasets and real images show the feasibility of the proposed method.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {deep learning,object detection,visual surveillance}
}

@article{kim2021generation,
  title = {Generation of {{Background Model Image Using Foreground Model}}},
  author = {Kim, Jae-Yeul and Ha, Jong-Eun},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {127515--127530},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3111686},
  urldate = {2025-10-03},
  abstract = {Proper consideration of the temporal domain and the spatial domain is essential to perform robust foreground object detection in visual surveillance. However, there are difficulties in considering long-term temporal information with CNN-based methods. To solve this limitation, classical algorithms and some deep learning-based algorithms have used a background model image. However, acquiring a sophisticated background model image is also one of the complex problems. Most of the algorithms take a lot of time to initialize the background model image and generate many errors in the presence of a static foreground. This paper proposes an algorithm for generating a background model image using a deep-learning-based segmenter to solve this problem. The proposed method shows a 66.25\% lower mean square error (MSE) than the background subtraction (BGS) algorithm and 79.25\% lower than the latest deep learning algorithm in the SBI dataset. In addition, in the deep learning-based segmenter that uses a background image as input, replacing the background image of BGS algorithm with the background image of the proposed method shows a 38.63\% reduction in the false detection rate (PWC).},
  keywords = {background model image,Classification algorithms,Feature extraction,foreground model,foreground object detection,Image segmentation,Object detection,Surveillance,Training,Visual surveillance,Visualization}
}

@article{kim2021spatiotemporal,
  title = {Spatio-{{Temporal Data Augmentation}} for {{Visual Surveillance}}},
  author = {Kim, Jae-Yeul and Ha, Jong-Eun},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {165014--165033},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3135505},
  urldate = {2025-10-03},
  abstract = {Visual surveillance aims to detect a foreground object using a continuous image acquired from a fixed camera. Recent deep learning methods based on supervised learning show superior performance compared to classical background subtraction algorithms. However, there is still room for improvement in the static foreground, dynamic background, hard shadow, illumination changes, camouflage, etc. In addition, most of the deep learning-based methods operate well in environments similar to training. If the testing environments are different from training ones, their performance degrades. As a result, additional training in those operating environments is required to ensure good performance. Our previous work, which uses spatio-temporal input data consisting of several past images, background images, and the current image, showed promising results in different environments from training. However, it uses a simple U-NET structure. This paper proposes a data augmentation technique suitable for visual surveillance for additional performance improvement using the same network used in our previous work. In deep learning, most data augmentation techniques deal with spatial-level data augmentation techniques used in image classification and object detection. We propose two data augmentation methods of adjusting background model images and past images. The proposed algorithm improves performance in complex areas such as static foreground and ghost objects compared to previous studies. Through quantitative and qualitative evaluation using SBI, LASIESTA, and our dataset, we show superior performance compared to deep learning-based algorithms and background subtraction algorithms. In addition, it has a 30.2\% and 27.9\% reduction of false detection rate in the LASIESTA and SBI dataset, respectively, compared to our previous study.},
  keywords = {Brightness,convolutional neural networks,data augmentation,Data models,deep learning,generalization power,Heuristic algorithms,Image sequences,Surveillance,Training,Visual surveillance,Visualization}
}

@article{kim2022foreground,
  title = {Foreground {{Object Detection}} in {{Visual Surveillance With Spatio-Temporal Fusion Network}}},
  author = {Kim, Jae-Yeul and Ha, Jong-Eun},
  year = {2022},
  journal = {IEEE Access},
  volume = {10},
  pages = {122857--122869},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3224063},
  urldate = {2025-10-03},
  abstract = {Object detection generally shows promising results only using spatial information, but foreground object detection in visual surveillance requires proper use of temporal information in addition to spatial information. Recently, deep learning-based visual surveillance algorithms have shown improved results, in an environment similar to training one, compared to traditional background subtraction (BGS) algorithms. However, in unseen environments, they show poor performance compared to BGS algorithms. This paper proposes an algorithm that improves performance in unseen environments by integrating spatial and temporal information. We propose a spatio-temporal fusion network (STFN) that extracts temporal and spatial information from 3D and 2D networks. Also, we propose a method for stable training of the proposed STFN using a semi-foreground map. STFN can generate a compliant background model image and operate in real-time on a desktop with GPU. The proposed algorithm performs well in an environment different from training and is demonstrated by experiments using various public datasets.},
  keywords = {Convolutional neural networks,deep learning,Deep learning,Feature extraction,foreground object detection,Object detection,spatio-temporal information,Spatiotemporal phenomena,Surveillance,Three-dimensional displays,Visual surveillance,Visualization}
}

@article{kim2022weakly,
  title = {Weakly {{Supervised Foreground Object Detection Network Using Background Model Image}}},
  author = {Kim, Jae-Yeul and Ha, Jong-Eun},
  year = {2022},
  journal = {IEEE Access},
  volume = {10},
  pages = {105726--105733},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3211987},
  urldate = {2025-10-03},
  abstract = {In visual surveillance, deep learning-based foreground object detection algorithms are superior to classical background subtraction (BGS)-based algorithms. However, deep learning-based methods are limited because detection performance deteriorates in a new environment different from the training environment. This limitation can be solved by retraining the model using additional ground-truth labels in the new environment. However, generating ground-truth labels for visual surveillance is time-consuming and expensive. This paper proposes a method that does not require foreground labels when adapting to a new environment. To this end, we propose an integrated network that produces two kinds of outputs a background model image and a foreground object map. We can adapt to the new environment by retraining using a background model image. The proposed method consists of one encoder and two decoders for detecting foreground objects and a background model image. It is designed to enable real-time processing with desktop GPUs. The proposed method shows 14.46\% improved FM in a new environment different from training and 11.49\% higher FM than the latest BGS algorithm.},
  keywords = {Data models,Decoding,deep learning,Deep learning,Feature extraction,foreground object detection,Object detection,Supervised learning,Surveillance,Visual surveillance,Visualization,weakly supervised}
}

@article{kim2023automatic,
  title = {Automatic {{Extrinsic Calibration}} of a {{Camera}} and a {{2D LiDAR With Point-Line Correspondences}}},
  author = {Kim, Jae-Yeul and Ha, Jong-Eun},
  year = {2023},
  journal = {IEEE Access},
  volume = {11},
  pages = {76904--76912},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2023.3298055},
  urldate = {2025-10-03},
  abstract = {Extrinsic calibration of a 2D camera and a 2D LiDAR is necessary to fuse information from two sensors by representing the information under the same frame. Various geometric constraints such as point-plane, point-line, and point-point are used for the extrinsic calibration. Usually, these require a manual step, including control points selection for camera calibration and LiDAR points. We propose a new algorithm for automatic extrinsic calibration with point-line correspondences. A calibration structure with two perpendicular planes having a chessboard on both sides is used for the extrinsic calibration. First, we use predefined colors at specific locations on a chessboard to quickly find the origin of the coordinate system. Second, we robustly detect three control points on LiDAR raw data using a geometric constraint that two end points among three control points should lie on the same line. The initial linear solution is obtained by using a point-line constraint. Finally, it is refined by nonlinear minimization, which gives a 15.3\% improvement compared to the linear solution. Experimental results show the feasibility of the proposed algorithm.},
  keywords = {Calibration,camera,Cameras,Extrinsic calibration,Image color analysis,Indexes,Laser radar,LiDAR,sensor fusion,Sensor fusion,Trajectory}
}

@article{kim2023msfnet,
  title = {{{MSF-NET}}: {{Foreground Objects Detection With Fusion}} of {{Motion}} and {{Semantic Features}}},
  shorttitle = {{{MSF-NET}}},
  author = {Kim, Jae-Yeul and Ha, Jong-Eun},
  year = {2023},
  journal = {IEEE Access},
  volume = {11},
  pages = {145551--145565},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2023.3345842},
  urldate = {2025-10-03},
  abstract = {Visual surveillance requires robust detection of foreground objects under challenging environments of abrupt lighting variation, stationary foreground objects, dynamic background objects, and severe weather conditions. Most classical algorithms leverage background model images produced by statistical modeling of the change of brightness values over time. Since they have difficulties using global features, many false detections occur at the stationary foreground regions and dynamic background objects. Recent deep learning-based methods can easily reflect global characteristics compared to classical methods. However, deep learning-based methods still need to be improved in utilizing spatiotemporal information. We propose an algorithm for efficiently using spatiotemporal information by adopting a split and merge framework. First, we split spatiotemporal information on successive multiple images into spatial and temporal parts using two sub-networks of semantic and motion networks. Finally, separated information is fused in a spatiotemporal fusion network. The proposed network consists of three sub-networks, which we note as MSF-NET (Motion and Semantic features Fusion NETwork). Also, we propose a method to train the proposed MSF-NET stably. Compared to the latest deep learning algorithms, the proposed MSF-NET gives 9\% and 13\% higher FM in the LASIESTA and SBI datasets. Also, we designed the proposed MSF-NET to be lightweight to run in real-time on a desktop GPU.},
  keywords = {Deep learning,foreground object detection,Heuristic algorithms,Object detection,Semantics,spatiotemporal information,Spatiotemporal phenomena,Surveillance,visual surveillance,Visualization}
}

@article{park2009easy,
  title = {Easy Calibration Method of Vision System for In-Situ Measurement of Strain of Thin Films},
  author = {Park, Jun-Hyub and Kang, Dong-Joong and Shin, Myung-Soo and Lim, Sung-Jo and Yu, Son-Cheol and Lee, Kwang-Soo and Ha, Jong-Eun and Choa, Sung-Hoon},
  year = {2009},
  month = sep,
  journal = {Transactions of Nonferrous Metals Society of China},
  volume = {19},
  pages = {s243-s249},
  issn = {1003-6326},
  doi = {10.1016/S1003-6326(10)60278-6},
  urldate = {2025-10-05},
  abstract = {An easy calibration method was presented for in-situ measurement of displacement in the order of nanometer during micro-tensile test for thin films by using CCD camera as a sensing device. The calibration of the sensing camera in the system is a central element part to measure displacement in the order of nanometer using images taken with the camera. This was accomplished by modeling the optical projection through the camera lens and relative locations between the object and camera in 3D space. A set of known 3D points on a plane where the film is located on is projected to an image plane as input data. These points, known as a calibration points, are then used to estimate the projection parameters of the camera. In the measurement system of the micro-scale by CCD camera, the calibration data acquisition and one-to-one matching steps between the image and 3D planes need precise data extraction procedures and repetitive user's operation to calibrate the measuring devices. The lack of the robust image feature extraction and easy matching prevent the practical use of these methods. A data selection method was proposed to overcome these limitations and offer an easy and convenient calibration of a vision system that has the CCD camera and the 3D reference plane with calibration marks of circular type on the surface of the plane. The method minimizes the user's intervention such as the fine tuning of illumination system and provides an efficient calibration method of the vision system for in-situ axial displacement measurement of the micro-tensile materials.},
  keywords = {adaptive binarization,automatic camera calibration,mechanical properties,plane homography,strain measurement,thin film}
}

@article{park2010improvement,
  title = {Improvement of Measurement Accuracy of Strain of Thin Film by Ccd Camera with a Template Matching Method Using the 2nd-Order Polynomial Interpolation},
  author = {Park, Jun-Hyub and Shin, Myung-Soo and Kang, Dong-Joong and Lim, Sung-Jo and Ha, Jong-Eun},
  year = {2010},
  month = jun,
  journal = {International Journal of Modern Physics B},
  volume = {24},
  number = {15n16},
  pages = {3101--3106},
  publisher = {World Scientific Publishing Co.},
  issn = {0217-9792},
  doi = {10.1142/S021797921006615X},
  urldate = {2025-10-05},
  abstract = {In this study, a system for non-contact in-situ measurement of strain during tensile test of thin films by using CCD camera with marking surface of specimen by black pen was implemented as a sensing device. To improve accuracy of measurement when CCD camera is used, this paper proposed a new method for measuring strain during tensile test of specimen with micrometer size. The size of pixel of CCD camera determines resolution of measurement, but the size of pixel can not satisfy the resolution required in tensile test of thin film because the extension of the specimen is very small during the tensile test. To increase resolution of measurement, the suggested method performs an accurate subpixel matching by applying 2nd order polynomial interpolation method to the conventional template matching. The algorithm was developed to calculate location of subpixel providing the best matching value by performing single dimensional polynomial interpolation from the results of pixel-based matching at a local region of image. The measurement resolution was less than 0.01 times of original pixel size. To verify the reliability of the system, the tensile test for the BeNi thin film was performed, which is widely used as a material in micro-probe tip. Tensile tests were performed and strains were measured using the proposed method and also the capacitance type displacement sensor for comparison. It is demonstrated that the new strain measurement system can effectively describe a behavior of materials after yield during the tensile test of the specimen at microscale with easy setup and better accuracy.},
  keywords = {mechanical properties,specimen,Strain,tensile test,thin film}
}

@article{song2016visual,
  title = {Visual Surveillance Using Wide-Angle Camera and Laser Range Finder},
  author = {Song, T.-H. and Ha, J.-E.},
  year = {2016},
  journal = {Electronics Letters},
  volume = {52},
  number = {6},
  pages = {445--447},
  issn = {1350-911X},
  doi = {10.1049/el.2016.0022},
  urldate = {2025-10-04},
  abstract = {In this Letter, a new algorithm to reduce false positive in video surveillance by integrating depth using laser range finder with conventional camera is proposed. Typical video surveillance detects foreground objects by background model using only image. Background model is updated using brightness value on image so that it has difficulty in modelling diverse variations caused by illumination changes or scene structure variations. Inherently, there could be many false positives. Hypothesis generation and verification paradigm is adopted by integrating laser range finder with camera on video surveillance. Conventional background updating algorithm is used for generating foreground objects. They are verified by depth value provided by laser range finder. Rotating 2D laser range finder for targeting and generation of look-up table is used. Experimental results show that proposed algorithm can reduce false positives in video surveillance.},
  copyright = {{\copyright} 2020 The Institution of Engineering and Technology},
  langid = {english},
  keywords = {background model,brightness value,foreground object detection,hypothesis generation,laser range finder,laser ranging,look-up table,table lookup,video surveillance,visual surveillance,wide-angle camera}
}

@incollection{uwamahoro2006attentive,
  title = {Attentive {{Person Selection}} for {{Human-Robot Interaction}}},
  booktitle = {Intelligent {{Computing}} in {{Signal Processing}} and {{Pattern Recognition}}: {{International Conference}} on {{Intelligent Computing}}, {{ICIC}} 2006 {{Kunming}}, {{China}}, {{August}} 16--19, 2006},
  author = {Uwamahoro, Diane Rurangirwa and Jeong, Mun-Ho and You, Bum-Jae and Ha, Jong-Eun and Kang, Dong-Joong},
  editor = {Huang, De-Shuang and Li, Kang and Irwin, George William},
  year = {2006},
  pages = {728--734},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-37258-5_83},
  urldate = {2025-10-05},
  abstract = {We present a method that enables the robot to select the most attentive person into communication from multiple persons, and gives its attention to the selected person. Our approach is a common components-based HMM where all HMM states share same components. Common components are probabilistic density functions of interaction distance and people's head direction toward the robot. In order to cope with the fact that the number of people in the robot's field of view is changeable, the number of states with common components can increase and decrease in our proposed model. In the experiments we used a humanoid robot with a binocular stereo camera. The robot considers people in its field of view at a given time and automatically shifts its attention to the person with highest probability. We confirmed that the proposed system works well in the selection of the attentive person to communicate with the robot.},
  isbn = {978-3-540-37258-5},
  langid = {english},
  keywords = {attention,Common components,Hidden Markov Model}
}

@article{wang2022dajung,
  title = {{다중 패치를 이용한 트랜스포머 기반 장면 텍스트 인식}},
  author = {Wang, Yao and 하종은},
  year = {2022},
  month = oct,
  journal = {제어로봇시스템학회 논문지},
  volume = {28},
  number = {10},
  pages = {862--867},
  issn = {1976-5622},
  doi = {10.5302/J.ICROS.2022.22.0107},
  urldate = {2025-10-03},
  abstract = {In this paper, we explore the application of Vision transformer (ViT) to the scene text recognition task. As a popular research direction in computer vision, Scene text recognition enables computers to recognize or read the text in natural scenes, such as object labels, text descriptions, and road text signs. At present, the traditional convolutional neural network-based model has better performance. Still, in the face of complex backgrounds and irregular scene text pictures, the performance of the convolutional neural network-based model is challenging to improve in curved text, diverse fonts, distortions, etc. With the application of transformers in computer vision, the model structure based on transformers has also significantly been developed. Although the current transformer-based model can obtain the performance of the model structure similar to CNN, it is currently in the early stage of application, and there is much room for research and improvement. We propose a multi-scale vertical rectangular patch model (MSVSTR) for transformer-based feature extractor to be more suitable for text images. By only arranging the patches in a single direction, when the image is cropped through the patch, it can be more suitable for the distribution form of the text in the text image. At the same time, to be suitable for different numbers of characters in other texts and more robust feature extraction, vertical rectangular patches of different scales are applied to crop the image. Our structure performs better through various ablation experiments than similar transformer-based STR models. At the same time, experiments show that our structure can perform seven benchmarks well.},
  langid = {korean},
  keywords = {Deep learning,Scene text recognition,Transformer}
}

@article{wang2023gaeyi,
  title = {{두 개의 인코더를 이용한 장면 텍스트 인식}},
  author = {Wang, Yao and 하종은},
  year = {2023},
  month = dec,
  journal = {제어로봇시스템학회 논문지},
  volume = {29},
  number = {12},
  pages = {973--979},
  issn = {1976-5622},
  doi = {10.5302/J.ICROS.2023.23.0146},
  urldate = {2025-10-03},
  abstract = {Despite significant advancements in scene text recognition, current models face substantial challenges, particularly when confronted with irregular text images featuring complex backgrounds, curved text, diverse fonts, and distortions. While convolutional neural network (CNN)-based text recognition networks have demonstrated commendable performance, they grapple with the aforementioned challenges. Recently, transformer-based feature extractors have exhibited advantages in global feature extraction from images, especially in the context of irregular text images. By employing self-attention, these transformers establish information connections between different parts of the image, thereby mitigating the impact of uneven character distribution. This study proposes multi-encoder scene text recognition (MESTR), a hybrid approach that combines a CNN-based and a transformer-based feature extractor. MESTR excels in simultaneously extracting local and global features from text images, ensuring the integration of both types of features to enhance performance. During training, we employed a guiding connectionist temporal classification (CTC) decoder [6] as a compensatory training strategy for the attentional decoder. Our experiments showed the efficacy of MESTR across seven benchmarks, demonstrating robust performance. In addition, ablation experiments are presented to validate the effectiveness of the proposed algorithm for scene text recognition.},
  langid = {korean},
  keywords = {convolutional neural network,deep learning,scene text recognition,transformer}
}

@article{yeomsangsig20202d,
  title = {{2D 시맨틱 분할 프로젝션을 이용한 3D 실내 공간 시맨틱 분할}},
  author = {염상식 and 하종은},
  year = {2020},
  month = nov,
  journal = {제어로봇시스템학회 논문지},
  volume = {26},
  number = {11},
  pages = {949--954},
  issn = {1976-5622},
  doi = {10.5302/J.ICROS.2020.20.0120},
  urldate = {2025-10-04},
  abstract = {Along with the development of 2D semantic segmentation in the pixel phase of 2D images using deep learning, the technique of detecting objects in 3D space is also emerging. Attempts to detect and split objects in 3D space are currently being actively carried out, but still show lower accuracy compared to 2D pixels. 3D point cloud data has the advantage of providing accurate distance relationship information for a given range of points, but it has irregular and unstructured limitations compared to segmentation within 2D pixel space. In this paper, the method of partitioning indoor point cloud data through 2D semantic segmentation is considered by adopting recently prosed point painting algorithm.},
  langid = {korean},
  keywords = {Deep Learning,Point Cloud,Semantic Segmentation}
}

@article{yeomsangsig2021imijiwa,
  title = {{이미지와 Voxel 을 이용한 3D 실내 점군 시맨틱 분할}},
  author = {염상식 and 하종은},
  year = {2021},
  month = dec,
  journal = {제어로봇시스템학회 논문지},
  volume = {27},
  number = {12},
  pages = {1000--1007},
  issn = {1976-5622},
  doi = {10.5302/J.ICROS.2021.21.0142},
  urldate = {2025-10-03},
  abstract = {In this paper, we propose a parallel network architecture that exhibits improved performance by fusing two-dimensional (2D) and three-dimensional (3D) features. A voxel-based and a projection-based method were adopted to derive the results through one scan. Our approach consists of two parallel networks, extracts features along each dimension, and converges them in a fusion network. In the fusion network, the voxel blocks and 2D feature maps extracted from each structure are fused to the voxel grid and then trained through convolution. For effective training of 2D networks, we use data augmentation techniques using coordinate system rotation transformation. In addition, a multi-loss with weights applied to each dimension was employed to effectively enhance the performance of the system, and the results revealed that the system exhibited better performance than when a single loss was used. Our proposed method can achieve better performance by changing the performance of the 2D network and 3D network, which can be generalized using other structures.},
  langid = {korean},
  keywords = {3D Vision,Point Cloud,Semantic Segmentation}
}
