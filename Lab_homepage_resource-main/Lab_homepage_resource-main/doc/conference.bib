@inproceedings{coehyeonseong2025noposplateul,
  title = {{NoPoSplat을 이용한 야외 환경의 조밀한 3D 복원}},
  booktitle = {{제어로봇시스템학회 국내학술대회 논문집}},
  author = {최현성 and 왕요 and 하종은},
  year = {2025},
  month = jun,
  pages = {394--395},
  urldate = {2025-10-10},
  abstract = {최현성, 왕요, 하종은 {\textbar} 제어로봇시스템학회 국내학술대회 논문집 {\textbar} 2025.6},
  langid = {korean}
}


@inproceedings{choi2011visual,
  title = {Visual Place Recognition Using Single Camera},
  booktitle = {2011 11th {{International Conference}} on {{Control}}, {{Automation}} and {{Systems}}},
  author = {Choi, I-Sak and Ha, Jong-Eun},
  year = {2011},
  month = oct,
  pages = {1137--1138},
  issn = {2093-7121},
  urldate = {2025-10-06},
  abstract = {This paper deals with visual place recognition by adopting visual categorization algorithm of bag of words. We aim to recognize different categories of room by applying bag of words algorithm. Experiments are done using real indoor images and it is shown that it can be used in the visual place recognition.},
  keywords = {Bag of visual word,Categorization,Computer vision,Databases,Dictionaries,Libraries,Place recognition,Vectors,Visualization}
}

@inproceedings{choi2019edge,
  title = {Edge Detection Using Deep Reinforcement Learning},
  booktitle = {2019 19th {{International Conference}} on {{Control}}, {{Automation}} and {{Systems}} ({{ICCAS}})},
  author = {Choi, Keong-Hun and Ha, Jong-Eun},
  year = {2019},
  pages = {97--98}
}

@inproceedings{choi2020visual,
  title = {Visual {{Surveillance}} Using {{Deep Reinforcement Learning}}},
  booktitle = {2020 20th {{International Conference}} on {{Control}}, {{Automation}} and {{Systems}} ({{ICCAS}})},
  author = {Choi, Keong-Hun and Ha, Jong-Eun},
  year = {2020},
  month = oct,
  pages = {289--291},
  issn = {2642-3901},
  doi = {10.23919/ICCAS50221.2020.9268429},
  urldate = {2025-10-05},
  abstract = {Visual surveillance aims a robust detection of foreground objects, and traditional algorithms usually use a background model image. A current is compared with the background model image. In this paper, we present a visual surveillance algorithm, which determines the parameters in Vibe using deep reinforcement learning. We apply DQN to determine three parameters in Vibe algorithm. We present a policy model which is composed of encoder and decoder type network. Experimental results shows the feasibility of the presented algorithm.},
  keywords = {BGS,Computational modeling,Data models,Deep learning,Frequency modulation,GAN,Reinforcement learning,Segmentation,Surveillance,Training,Visual surveillance,Visualization}
}

@inproceedings{choi2022random,
  title = {Random {{Swin Transformer}}},
  booktitle = {2022 22nd {{International Conference}} on {{Control}}, {{Automation}} and {{Systems}} ({{ICCAS}})},
  author = {Choi, Keong-Hun and Ha, Jong-Eun},
  year = {2022},
  month = jan,
  pages = {1611--1614},
  issn = {2642-3901},
  doi = {10.23919/ICCAS55662.2022.10003789},
  urldate = {2025-10-05},
  abstract = {After deep learning appeared, the convolutional neural network (CNN) dominated various applications of image classification, object detection, and semantic segmentation. Recently, a transformer based on various attention mechanisms performed better than the CNN. But, the transformer requires a large amount of memory for full attention among tokens. Recently, a Swin transformer has been proposed to solve that memory issue. It applies the attention per sub-regions on an image. Also, it solves a problem caused by not using full attention on an image by shifting window that guarantees more tokens are involved in attention. In this paper, we investigate a method of randomly selecting tokens in Swin transformer. We randomly choose tokens within a certain range rather than using a fixed shift value in the Swin transformer. Experimental results show the feasibility of the proposed method.},
  keywords = {Automation,Classification,Control systems,Deep learning,Memory management,Object detection,Semantic segmentation,Swin transformer,Transformer,Transformers}
}

@inproceedings{choi2022semantic,
  title = {Semantic {{Segmentation}} with {{Perceiver IO}}},
  booktitle = {2022 22nd {{International Conference}} on {{Control}}, {{Automation}} and {{Systems}} ({{ICCAS}})},
  author = {Choi, Keong-Hun and Ha, Jong-Eun},
  year = {2022},
  month = jan,
  pages = {1607--1610},
  issn = {2642-3901},
  doi = {10.23919/ICCAS55662.2022.10003862},
  urldate = {2025-10-05},
  abstract = {Recently, in deep learning, the transformer is replacing the convolutional neural network (CNN) due to its performance and simple design. In particular, in recent studies, constructing an encoder of the transformer that effectively extracts features on an image has been widely used. However, even in these cases, models utilizing existing deep neural network structures needed to use a form suitable for each data format according to input modality. Recently, the Perceiver IO [6] has been proposed to overcome this limitation. It can process various data formats through one structure to extract a characteristic value. Also, it uses an output query to output data as we want. In this paper, a semantic segmentation model using the characteristics of the Perceiver IO is presented. Two types of input configuration are suggested, and experimental results show the feasibility of the proposed method.},
  keywords = {Data models,Deep learning,Deep learning.,Feature extraction,Neural networks,Perceiver IO,Semantic segmentation,Semantics,Transformers}
}

@inproceedings{choi2023object,
  title = {Object {{Detection Using Policy-Based Reinforcement Learning}}},
  booktitle = {2023 23rd {{International Conference}} on {{Control}}, {{Automation}} and {{Systems}} ({{ICCAS}})},
  author = {Choi, Keong-Hun and Ha, Jong-Eun},
  year = {2023},
  month = oct,
  pages = {235--237},
  issn = {2642-3901},
  doi = {10.23919/ICCAS59377.2023.10316786},
  urldate = {2025-10-05},
  abstract = {By constructing an object detection model using a deep neural network structure, it was possible to obtain faster and more accurate results than traditional models. Afterwards, through the method of applying the transformer structure by replacing the existing convolutional layer, the model structure, which was previously carried out in two stages, was simplified, and it became possible to detect the size of the object more freely. However, there are difficulties in generating new data due to the complex configuration of data used for training. As a result, a lot of resources are consumed in data generation, and it is difficult to train in response to a new environment immediately. This study presents a method for learning using simpler data. The simplified data consisted of the input image and the number of objects to be detected. To train using the data, a reinforcement learning method was applied to evaluate the output of the detection model and create and train a reward based on this.},
  keywords = {Adaptation models,Artificial neural networks,Automation,Deep learning,Object detection,Reinforcement learning,Training,Transformer,Transformers}
}

@inproceedings{coegyeonghun2020ejia,
  title = {{에지 비용 함수를 이용한 시맨틱 분할 **[Poster]**}},
  booktitle = {{제어로봇시스템학회 국내학술대회 논문집}},
  author = {최경훈 and 하종은},
  year = {2020},
  month = jul,
  pages = {357--358},
  urldate = {2025-10-06},
  abstract = {최경훈, 하종은 {\textbar} 제어로봇시스템학회 국내학술대회 논문집 {\textbar} 2020.7},
  langid = {korean},
  keywords = {Convolutional Neural Networks,Deep Learning,Segmentation},
  annotation = {bibtex[title] = 에지 비용 함수를 이용한 시맨틱 분할}
}

@inproceedings{coeisag20112d,
  title = {{2D 와 3D 정보를 이용한 영상 교시기반 주행 알고리듬 성능 비교}},
  booktitle = {{제어로봇시스템학회 국내학술대회 논문집}},
  author = {최이삭 and 하종은},
  year = {2011},
  month = may,
  pages = {275--280},
  urldate = {2025-10-06},
  abstract = {This paper deals with performance comparison visual path following using 2D and 3D information. Visual path follow first teaches driving path by selecting milestone images then follows the same route by comparing the milestone image and current image. We follow the visual path following algorithm of [8] and [10]. In [8], a robot navigated with 2D image information only. But In [10], Local 3D geometries are reconstructed between the milestone images in order to achieve fast feature prediction which allows the recovery from tracking failures. Experimental results including diverse indoor cases show performance of each algorithm.},
  langid = {korean},
  keywords = {Autonomous Navigation,Feature Matching,Intelligent Robot,Path Follow}
}

@inproceedings{gimdaehun2017dibreoningeul,
  title = {{딥러닝을 이용한 다차선 검출}},
  booktitle = {{제어로봇시스템학회 국내학술대회 논문집}},
  author = {김대훈 and 하종은},
  year = {2017},
  month = may,
  pages = {310--311},
  urldate = {2025-10-05},
  abstract = {김대훈, 하종은 {\textbar} 제어로봇시스템학회 국내학술대회 논문집 {\textbar} 2017.5},
  langid = {korean},
  keywords = {Convolutional neural networks,Deep Learning,Lane detection}
}

@inproceedings{gimdaehun2018gaeyi,
  title = {{두 개의 이종 CNN 융합을 이용한 차선 검출 **[Poster]**}},
  booktitle = {{제어로봇시스템학회 국내학술대회 논문집}},
  author = {김대훈 and 하종은},
  year = {2018},
  month = may,
  pages = {418--419},
  urldate = {2025-10-05},
  abstract = {김대훈, 하종은 {\textbar} 제어로봇시스템학회 국내학술대회 논문집 {\textbar} 2018.5},
  langid = {korean},
  keywords = {Convolutional Neural Networks,Deep Learning,Lane Detection},
  annotation = {bibtex[title] = 두 개의 이종 CNN 융합을 이용한 차선 검출}
}

@inproceedings{gimdonghwan2012robospaleul,
  title = {{로봇팔을 이용한 삼차원 기반 물체 조작}},
  booktitle = {{제어로봇시스템학회 국내학술대회 논문집}},
  author = {김동환 and 허강욱 and 최이삭 and 하종은},
  year = {2012},
  month = apr,
  pages = {117--118},
  urldate = {2025-10-05},
  abstract = {김동환, 허강욱, 최이삭, 하종은 {\textbar} 제어로봇시스템학회 국내학술대회 논문집 {\textbar} 2012.4},
  langid = {korean},
  keywords = {inverse kinematics,kinect,object recognition,robot arm}
}

@inproceedings{gimdonghwan2013nasa,
  title = {나사 머리부 불량 검출 알고리듬},
  booktitle = {대한기계학회 2013년도 학술대회},
  author = {{김동환} and {김진우} and {하종은}},
  year = {2013}
}

@inproceedings{gimjaeyeol2020vae,
  title = {{VAE 로 생성한 배경 이미지를 이용한 무인감시}},
  booktitle = {{제어로봇시스템학회 국내학술대회 논문집}},
  author = {김재열 and 하종은},
  year = {2020},
  month = jul,
  pages = {355--356},
  urldate = {2025-10-05},
  abstract = {김재열, 하종은 {\textbar} 제어로봇시스템학회 국내학술대회 논문집 {\textbar} 2020.7},
  langid = {korean}
}

@inproceedings{gimjinu2014kalraa,
  title = {{칼라 나사 검사 알고리듬}},
  booktitle = {{한국정밀공학회 학술발표대회 논문집}},
  author = {김진우 and 하종은},
  year = {2014},
  month = oct,
  pages = {812--812},
  urldate = {2025-10-06},
  abstract = {김진우, 하종은 {\textbar} 한국정밀공학회 학술발표대회 논문집 {\textbar} 2014.10},
  langid = {korean},
  keywords = {Color Fastener,Inspection,Machine Vision}
}

@inproceedings{gimjinu2015oebua,
  title = {{외부 보정을 이용한 PTZ 카메라 보정}},
  booktitle = {{제어로봇시스템학회 국내학술대회 논문집}},
  author = {김진우 and 송태훈 and 하종은},
  year = {2015},
  month = may,
  pages = {167--168},
  urldate = {2025-10-06},
  abstract = {김진우, 송태훈, 하종은 {\textbar} 제어로봇시스템학회 국내학술대회 논문집 {\textbar} 2015.5},
  langid = {korean},
  keywords = {extrinsic calibration,hand/eye calibration,PTZ camera}
}

@inproceedings{gimjinu2015seulris,
  title = {슬릿 레이저를 이용한 나사 {{3D}} 형상 측정},
  booktitle = {한국정밀공학회 2015년도 춘계학술대회 논문집},
  author = {{김진우} and {하종은}},
  year = {2015},
  month = may,
  pages = {1221--1222}
}

@inproceedings{gimjinu2016seutereo,
  title = {{스테레오 시스템을 이용한 SLAM}},
  booktitle = {{제어로봇시스템학회 국내학술대회 논문집}},
  author = {김진우 and 하종은},
  year = {2016},
  month = mar,
  pages = {51--52},
  urldate = {2025-10-05},
  abstract = {김진우, 하종은 {\textbar} 제어로봇시스템학회 국내학술대회 논문집 {\textbar} 2016.3},
  langid = {korean},
  keywords = {autonomous vehicle,SLAM,stereo vision}
}

@inproceedings{gimnamhun2016yeongsangceorireul,
  title = {{영상처리를 통한 소형 휴머노이드의 임무 수행}},
  booktitle = {{제어로봇시스템학회 국내학술대회 논문집}},
  author = {김남훈 and 하종은},
  year = {2016},
  month = mar,
  pages = {327--328},
  urldate = {2025-10-05},
  abstract = {김남훈, 하종은 {\textbar} 제어로봇시스템학회 국내학술대회 논문집 {\textbar} 2016.3},
  langid = {korean},
  keywords = {Autonomy drive,Localization,LSM}
}

@inproceedings{gimnamhun2017sujig,
  title = {{수직 구조물을 이용한 카메라와 라이다 외부 보정}},
  booktitle = {{제어로봇시스템학회 국내학술대회 논문집}},
  author = {김남훈 and 하종은},
  year = {2017},
  month = may,
  pages = {308--309},
  urldate = {2025-10-05},
  abstract = {김남훈, 하종은 {\textbar} 제어로봇시스템학회 국내학술대회 논문집 {\textbar} 2017.5},
  langid = {korean}
}

@inproceedings{ha2004initialization,
  title = {Initialization {{Method}} for the {{Self-Calibration Using Minimal Two Images}}},
  booktitle = {Computational {{Science}} and {{Its Applications}} -- {{ICCSA}} 2004},
  author = {Ha, Jong-Eun and Kang, Dong-Joong},
  editor = {Lagan{\'a}, Antonio and Gavrilova, Marina L. and Kumar, Vipin and Mun, Youngsong and Tan, C. J. Kenneth and Gervasi, Osvaldo},
  year = {2004},
  pages = {915--923},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-24768-5_98},
  abstract = {Recently, 3D structure recovery through self-calibration of camera has been actively researched. Traditional calibration algorithm requires known 3D coordinates of the control points while self-calibration only requires the corresponding points of images, thus it has more flexibility in real application. In general, self-calibration algorithm results in the nonlinear optimization problem using constraints from the intrinsic parameters of the camera. Thus, it requires initial value for the nonlinear minimization. Traditional approaches get the initial values assuming they have the same intrinsic parameters while they are dealing with the situation where the intrinsic parameters of the camera may change. In this paper, we propose new initialization method using the minimum 2 images. Proposed method is based on the assumption that the least violation of the camera's intrinsic parameter gives more stable initial value. Synthetic and real experiment shows this result.},
  isbn = {978-3-540-24768-5},
  langid = {english}
}

@inproceedings{ha2005fast,
  title = {Fast {{Candidate Generation}} for {{Template Matching Using Two}} 1-{{D Edge Projections}}},
  booktitle = {{{AI}} 2005: {{Advances}} in {{Artificial Intelligence}}},
  author = {Ha, Jong-Eun and Kang, Dong-Joong},
  editor = {Zhang, Shichao and Jarvis, Ray},
  year = {2005},
  pages = {1267--1271},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/11589990_184},
  abstract = {In machine vision, template matching is key component and used usefully in various tasks such as pick and place, mark identification, and alignment. In this paper, we propose fast template matching algorithm using edge projection. Proposed algorithm reduces the search problem from 2D into 1D using edge projection within the 2D template area. By this, it could effectively reduce the computational burden. Also, it gives comparable discriminating power compared to template matching using intensity. In this paper, rotation and translation search is implemented to cope with typical machine vision application where the height between camera and target object is fixed.},
  isbn = {978-3-540-31652-7},
  langid = {english},
  keywords = {Model Image,Pyramid Image,Scale Invariant Feature Transform,Target Image,Template Match}
}

@inproceedings{ha2005sequential,
  title = {Sequential {{Robust Direct Motion Estimation}} with {{Equal Projective Basis}}},
  booktitle = {Advances in {{Multimedia Information Processing}} - {{PCM}} 2004},
  author = {Ha, Jong-Eun and Kang, Dong-Joong and Jeong, Muh-Ho},
  editor = {Aizawa, Kiyoharu and Nakamura, Yuichi and Satoh, Shin'ichi},
  year = {2005},
  pages = {214--221},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-30541-5_27},
  abstract = {This paper presents sequential robust direct motion estimation algorithm for the processing of uncalibrated dynamic image sequences. Through the propagation of the projective structures of the first two frames, we guarantee projective information under an equal basis. Since proposed algorithm yields projective information under equal basis, we could directly use these outputs for the metric recovery through self-calibration and motion segmentation using trifocal tenor.},
  isbn = {978-3-540-30541-5},
  langid = {english},
  keywords = {projective basis,sequential motion estimation,Uncalibrated}
}

@inproceedings{hajongeun2011jeonjabim,
  title = {{전자빔 가공 패턴 정밀 측정 알고리듬}},
  booktitle = {{한국정밀공학회 학술발표대회 논문집}},
  author = {하종은 and 최이삭 and 허강욱},
  year = {2011},
  month = jun,
  pages = {781--782},
  urldate = {2025-10-06},
  abstract = {하종은, 최이삭, 허강욱 {\textbar} 한국정밀공학회 학술발표대회 논문집 {\textbar} 2011.6},
  langid = {korean},
  keywords = {Electron Beam,Measurement,Subpixel}
}

@inproceedings{hajongeun2014torx,
  title = {{Torx 형상 나사머리 검사 알고리듬}},
  booktitle = {{대한기계학회 2014년도 생산 및 설계공학부문 춘계학술대회 논문집}},
  author = {하종은 and 나승우},
  year = {2014},
  month = apr,
  pages = {3--4},
  urldate = {2025-10-05},
  abstract = {하종은, 나승우 {\textbar} 대한기계학회 춘추학술대회 {\textbar} 2014.4},
  langid = {korean}
}

@inproceedings{heogangug2011sayeong,
  title = {{사영 모델 기반 SEM 보정}},
  booktitle = {{한국정밀공학회 학술발표대회 논문집}},
  author = {허강욱 and 최이삭 and 하종은},
  year = {2011},
  month = oct,
  pages = {455--456},
  urldate = {2025-10-06},
  abstract = {허강욱, 최이삭, 하종은 {\textbar} 한국정밀공학회 학술발표대회 논문집 {\textbar} 2011.10},
  langid = {korean},
  keywords = {Calibration,Projective,SEM}
}

@inproceedings{her2011steering,
  title = {Steering Angle Determination Using {{CART}} and Voting},
  booktitle = {2011 11th {{International Conference}} on {{Control}}, {{Automation}} and {{Systems}}},
  author = {Her, Kang-Wook and Ha, Jong-Eun and Lee, Wang-Heon},
  year = {2011},
  month = oct,
  pages = {1139--1141},
  issn = {2093-7121},
  urldate = {2025-10-06},
  abstract = {In autonomous navigation using camera, it is important to robustly find drivable region. After finding drivable region, we compute the steering angle for the vehicle. In this paper, we propose a robust algorithm for finding the drivable region. First, we find drivable region using classification and regression tree (CART). Finally, we find the epipole of the road using voting. We show the feasibility and robustness of proposed algorithm in experiment using real images.},
  keywords = {Autonomous Navigation,Calibration,Cameras,CART,Navigation,Road Detection,Roads,Robustness,Shape,Steering Angle,Vehicles}
}

@inproceedings{her2012localization,
  title = {Localization of Mobile Robot Using Laser Range Finder and {{IR}} Landmark},
  booktitle = {2012 12th {{International Conference}} on {{Control}}, {{Automation}} and {{Systems}}},
  author = {Her, Kang-Wook and Kim, Dong-Hwan and Ha, Jong-Eun},
  year = {2012},
  month = oct,
  pages = {459--461},
  urldate = {2025-10-05},
  abstract = {Localization and mapping is important in autonomous navigation of mobile robot. In this paper, we deal with localization using 2D laser range finder and IR positioning sensor. 2D grid map is built in local coordinate using data from laser range finder. And it is integrated into global coordinate using information from IR landmark. It can provide consistent global map that can be used in the autonomous navigation of mobile robot. Experimental result under indoor environment is provided to show the feasibility.},
  keywords = {Cameras,Landmark,Lasers,Localization,Mobile Robot,Mobile robots,Navigation,Robot kinematics,Robot vision systems}
}

@inproceedings{jang2018extrinsic,
  title = {Extrinsic Calibration of {{RGB-D}} Sensor and Robot Using Color Chessboard},
  booktitle = {2018 18th {{International Conference}} on {{Control}}, {{Automation}} and {{Systems}} ({{ICCAS}})},
  author = {Jang, Kyeong-Seock and Ha, Jong-Eun},
  year = {2018},
  pages = {1455--1456}
}

@inproceedings{jang2018extrinsica,
  title = {Extrinsic Calibration of Camera and Laser Range Finder Using Dummy Camera without {{IR}} Cut-Filter},
  booktitle = {2018 18th {{International Conference}} on {{Control}}, {{Automation}} and {{Systems}} ({{ICCAS}})},
  author = {Jang, Kyeong-Seock and Ha, Jong-Eun},
  year = {2018},
  pages = {1457--1458}
}

@inproceedings{janggyeongseog2018rgbd,
  title = {{RGB-D 센서를 이용한 로봇의 물체 조작}},
  booktitle = {{제어로봇시스템학회 국내학술대회 논문집}},
  author = {장경석 and 하종은},
  year = {2018},
  month = may,
  pages = {426--427},
  urldate = {2025-10-05},
  abstract = {장경석, 하종은 {\textbar} 제어로봇시스템학회 국내학술대회 논문집 {\textbar} 2018.5},
  langid = {korean},
  keywords = {Euclidean segmentation,Object Manipulation,RGB-D Sensor,Robot}
}

@inproceedings{kang2004object,
  title = {Object {{Mark Segmentation Algorithm Using Dynamic Programming}} for {{Poor Quality Images}} in {{Automated Inspection Process}}},
  booktitle = {Computational {{Science}} and {{Its Applications}} -- {{ICCSA}} 2004},
  author = {Kang, Dong-Joong and Ha, Jong-Eun and Ahn, In-Mo},
  editor = {Lagan{\'a}, Antonio and Gavrilova, Marina L. and Kumar, Vipin and Mun, Youngsong and Tan, C. J. Kenneth and Gervasi, Osvaldo},
  year = {2004},
  pages = {896--905},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-24768-5_96},
  abstract = {This paper presents a method to segment object ID (identification) marks on poor quality images under uncontrolled lighting conditions of automated inspection process. The method is based on multiple templates and normalized gray-level correlation (NGC) method. We propose a multiple template method, called as ATM (Active Template Model) which uses a search technique of multiple templates from model templates to match and segment character regions of the inspection images. Conventional Snakes algorithm provides a good methodology to model the functional of ATM. To increase the computation speed to segment the ID mark regions, we introduce the Dynamic Programming based algorithm. Experimental results using real images from automated factory are presented.},
  isbn = {978-3-540-24768-5},
  langid = {english},
  keywords = {Active Contour,Active Contour Model,Model Template,Optical Character Recognition,Poor Quality Image}
}

@inproceedings{kang2005polyhedral,
  title = {A {{Polyhedral Object Recognition Algorithm}} for {{Augmented Reality}}},
  booktitle = {Advances in {{Multimedia Information Processing}} - {{PCM}} 2004},
  author = {Kang, Dong-Joong and Ha, Jong-Eun and Jeong, Muh-Ho},
  editor = {Aizawa, Kiyoharu and Nakamura, Yuichi and Satoh, Shin'ichi},
  year = {2005},
  pages = {402--409},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-30541-5_50},
  abstract = {Registration between cameras and objects is a central element for augmented reality applications and required to combine real and rendered scenes. In this paper, we present a new approach to solve the problem of estimating the camera 3-D location and orientation from a matched set of 3-D model and 2-D image features. An iterative least-square method is used to solve both rotation and translation simultaneously. We derive an error equation using roll-pitch-yaw angle to present the rotation matrix. From the modeling of an error equation, we analytically extract the partial derivates for estimation parameters from the nonlinear error equation. To minimize the error equation, Levenberg-Marquardt algorithm is introduced with uniform sampling strategy of rotation space to avoid stuck in local minimum.},
  isbn = {978-3-540-30541-5},
  langid = {english},
  keywords = {augmented reality,computer vision,Nonlinear optimization,polyhedral object recognition,pose estimation}
}

@inproceedings{kang2006fast,
  title = {A {{Fast Method}} for {{Detecting Moving Vehicles Using Plane Constraint}} of {{Geometric Invariance}}},
  booktitle = {Computational {{Science}} and {{Its Applications}} - {{ICCSA}} 2006},
  author = {Kang, Dong-Joong and Ha, Jong-Eun and Lho, Tae-Jung},
  editor = {Gavrilova, Marina and Gervasi, Osvaldo and Kumar, Vipin and Tan, C. J. Kenneth and Taniar, David and Lagan{\'a}, Antonio and Mun, Youngsong and Choo, Hyunseung},
  year = {2006},
  pages = {1163--1171},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/11751595_122},
  abstract = {This paper presents a new method of detecting on-road highway vehicles for active safety vehicle system. We combine a projective invariant technique with motion information to detect overtaking road vehicles. The vehicles are assumed into a set of planes and the invariant technique extracts the plane from the theory that a geometric invariant value defined by five points on a plane is preserved under a projective transform. Harris corners as a salient image point are used to give motion information with the normalized cross correlation centered at these points. A probabilistic criterion without demand of a heuristic factor is defined to test the similarity of invariant values between sequential frames. Because the method is very fast, real-time processing is possible for vehicle detection. Experimental results using images of real road scenes are presented.},
  isbn = {978-3-540-34076-8},
  langid = {english},
  keywords = {active safety vehicle,Geometric invariant,motion,plane constraint,vehicle detection}
}

@inproceedings{keonghun2021segmentation,
  title = {Segmentation Applying {{TAG}} Type Label Data and {{Transformer}}},
  booktitle = {2021 21st {{International Conference}} on {{Control}}, {{Automation}} and {{Systems}} ({{ICCAS}})},
  author = {Keonghun, Choi and Ha, Jong--Eun},
  year = {2021},
  month = oct,
  pages = {1519--1522},
  issn = {2642-3901},
  doi = {10.23919/ICCAS52745.2021.9650042},
  urldate = {2025-10-05},
  abstract = {Autonomous driving of vehicles or robots using artificial intelligence is being studied the most. The recognition of the surrounding environment is the basis for artificial intelligence that requires interaction with the surroundings, which means that research on object detection is necessary. The size of the model is smaller, and more information can be obtained than detection using anchors, but the accuracy of segmentation is generally lower. In this paper, to improve this point, a transformed transformer structure is applied to improve the performance of segmentation, and it is proposed to use data in a format different from the existing label data. By using a single image as an input, there is no loss of location information, and a lighter model is presented by obtaining a segmentation image without going through a separate process. At the same time, to improve generalization performance, a method of assigning one label to one characteristic rather than assigning one label to one object was applied to the composition of the label data, and the difference in generalization ability was compared.},
  keywords = {Artificial intelligence,Automation,Control systems,Deep learning,Image segmentation,Object detection,Segmentation,Surveillance,Transformer,Transformers,Visual surveillance}
}

@inproceedings{keonghun2021visual,
  title = {Visual Surveillance Transformer},
  booktitle = {2021 21st {{International Conference}} on {{Control}}, {{Automation}} and {{Systems}} ({{ICCAS}})},
  author = {Keonghun, Choi and Ha, Jong-Eun},
  year = {2021},
  month = oct,
  pages = {1516--1518},
  issn = {2642-3901},
  doi = {10.23919/ICCAS52745.2021.9649823},
  urldate = {2025-10-05},
  abstract = {In the case of the unmanned surveillance system field, even if it is the same object, the detection result will be different depending on the state of the object and the configuration of the surrounding environment. Therefore, artificial intelligence for unmanned surveillance needs to understand the environment on the image, understand the state of the object within the image, and understand the relationship between them. For this purpose, in this study, a transformed transformer structure that can receive a single image, which is 2D data, as an input, unlike splitting one image into a certain size and using it as an input, is presented, and the effect between neighboring pixels is considered by using a segmentation model to which it is applied. A possible background classification model was constructed.},
  keywords = {Automation,Control systems,Data models,Deep learning,Image segmentation,Segmentation,Surveillance,Transformer,Transformers,Visual surveillance,Visualization}
}

@inproceedings{kim2016extrinsic,
  title = {Extrinsic Calibration of a Camera and Laser Range Finder},
  booktitle = {2016 16th {{International Conference}} on {{Control}}, {{Automation}} and {{Systems}} ({{ICCAS}})},
  author = {Kim, Nam-Hun and Ha, Jong-Eun},
  year = {2016}
}

@inproceedings{kim2016mapping,
  title = {Mapping Using Stereo System},
  booktitle = {2016 16th {{International Conference}} on {{Control}}, {{Automation}} and {{Systems}} ({{ICCAS}})},
  author = {Kim, Jin-Woo and Lee, Wang-Heon and Ha, Jong-Eun},
  year = {2016}
}

@inproceedings{kim2017multilane,
  title = {Multi-Lane Detection by Finding Epipole Using Convolutional Neural Network},
  booktitle = {2017 17th {{International Conference}} on {{Control}}, {{Automation}} and {{Systems}} ({{ICCAS}})},
  author = {Kim, D. H. and Ha, J. E.},
  year = {2017}
}

@inproceedings{kim2017zooming,
  title = {Zooming Multiple Objects Using Wide-Angle Camera and {{PTZ}} Camera in Visual Surveillance},
  booktitle = {2017 17th {{International Conference}} on {{Control}}, {{Automation}} and {{Systems}} ({{ICCAS}})},
  author = {Kim, N. H. and Ha, J. E.},
  year = {2017}
}

@inproceedings{kim2018lane,
  title = {Lane Detection Using Deep Learning Semantic Segmentation},
  booktitle = {2018 18th {{International Conference}} on {{Control}}, {{Automation}} and {{Systems}} ({{ICCAS}})},
  author = {Kim, Dae-Hun and Ha, Jong-Eun},
  year = {2018},
  pages = {1459--1460}
}

@inproceedings{kim2019automatic,
  title = {Automatic Extrinsic Calibration of a Camera and Laser Range Finder Using Constraints Fusion},
  booktitle = {2019 19th {{International Conference}} on {{Control}}, {{Automation}} and {{Systems}} ({{ICCAS}})},
  author = {Kim, Jae-Yeul and Ha, Jong-Eun},
  year = {2019},
  pages = {317--318}
}

@inproceedings{kim2020visual,
  title = {Visual {{Surveillance}} Using {{Background Model Image Generated}} by {{GAN}}},
  booktitle = {2020 20th {{International Conference}} on {{Control}}, {{Automation}} and {{Systems}} ({{ICCAS}})},
  author = {Kim, Jae-Yeul and Ha, Jong-Eun},
  year = {2020},
  month = oct,
  pages = {292--295},
  issn = {2642-3901},
  doi = {10.23919/ICCAS50221.2020.9268373},
  urldate = {2025-10-05},
  abstract = {Visual surveillance requires robust foreground and background separation capabilities in various environments. Although various traditional algorithms based on background subtraction methods have been proposed, problems such as hard shadows, camouflage, and ghost effects remain. Recently, deep learning-based foreground detection methods have been proposed. Deep learning-based methods outperform traditional algorithms in various unmanned surveillance datasets. However, even deep learning-based methods show insufficient generalization ability in certain datasets. For data that have not been trained, a number of errors are detected. Even among deep learning-based methods, there are methods that show higher generalization ability by using a background image. In this paper, we propose a method of using GAN to generate background images.},
  keywords = {BGS,Deep learning,GAN,Generative adversarial networks,Generators,Learning systems,Road transportation,Segmentation,Surveillance,Training,Visual surveillance,Visualization}
}

@inproceedings{lee20173d,
  title = {{{3D}} Space Recognition for Indoor Drone Navigation Using Multiple Cameras},
  booktitle = {2017 17th {{International Conference}} on {{Control}}, {{Automation}} and {{Systems}} ({{ICCAS}})},
  author = {Lee, W. H. and Ha, J. E. and Oh, K. W.},
  year = {2017}
}

@inproceedings{lho2004linebased,
  title = {A {{Line-Based Pose Estimation Algorithm}} for 3-{{D Polyhedral Object Recognition}}},
  booktitle = {Computational {{Science}} and {{Its Applications}} -- {{ICCSA}} 2004},
  author = {Lho, Tae-Jung and Kang, Dong-Joong and Ha, Jong-Eun},
  editor = {Lagan{\'a}, Antonio and Gavrilova, Marina L. and Kumar, Vipin and Mun, Youngsong and Tan, C. J. Kenneth and Gervasi, Osvaldo},
  year = {2004},
  pages = {906--914},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-540-24768-5_97},
  abstract = {In this paper, we present a new approach to solve the problem of estimating the camera 3-D location and orientation from a matched set of 3-D model and 2-D image features. An iterative least-square method is used to solve both rotation and translation simultaneously. Because conventional methods that solved for rotation first and then translation do not provide good solutions, we derive an error equation using roll-pitch-yaw angle to present the rotation matrix. From the modeling of the error equation, we analytically extract the partial derivates for estimation parameters from the nonlinear error equation. To minimize the error equation, Levenberg-Marquardt algorithm is introduced with uniform sampling strategy of rotation space to avoid stuck in local minimum. Experimental results using real images are presented.},
  isbn = {978-3-540-24768-5},
  langid = {english},
  keywords = {Error Equation,Image Line,Model Line,Object Recognition,Real Image}
}

@inproceedings{vavilin2012automatic,
  title = {Automatic {{Context Analysis}} for {{Image Classification}} and {{Retrieval}}},
  booktitle = {Advanced {{Intelligent Computing}}},
  author = {Vavilin, Andrey and Jo, Kang-Hyun and Jeong, Moon-Ho and Ha, Jong-Eun and Kang, Dong-Joong},
  editor = {Huang, De-Shuang and Gan, Yong and Bevilacqua, Vitoantonio and Figueroa, Juan Carlos},
  year = {2012},
  pages = {377--382},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-24728-6_51},
  abstract = {This paper describes a method for image classification and retrieval for natural and urban scenes. The proposed algorithm is based on hierarchical image contents analysis. First image is classified as urban or natural according to color and edge distribution properties. Additionally scene is classified according to its conditions: illumination, weather, season and daytime based on contrast, saturation and color properties of the image. Then image content is analyzed in order to detect specific object classes: buildings, cars, trees, sky, road etc. To do so, image recursively divided into rectangular blocks. For each block probabilities of membership in the specific class is computed. This probability computed as a distance in a feature space defined by optimal feature subset selected on the training step. Blocks which can not be assigned to any class using computed features are separated into 4 sub-blocks which analyzed recursively. Process stopped then all blocks are classified or size of block is smaller then predefined value. Training process is used to select optimal feature subset for object classification. Training set contains images with manually labeled objects of different classes. Each image additionally tagged with scene parameters (illumination, weather etc).},
  isbn = {978-3-642-24728-6},
  langid = {english},
  keywords = {Image Classification,Image Retrieval,Optimal Feature Subset,Scene Classification,Scene Condition}
}

@inproceedings{wang2020simceung,
  title = {{심층 신경망을 이용한 장면 텍스트 추출 및 인식}},
  booktitle = {{제어로봇시스템학회 국내학술대회 논문집}},
  author = {Wang, Yao and 하종은},
  year = {2020},
  month = jul,
  pages = {410--411},
  urldate = {2025-10-05},
  abstract = {Yao Wang, 하종은 {\textbar} 제어로봇시스템학회 국내학술대회 논문집 {\textbar} 2020.7},
  langid = {korean},
  keywords = {computer vision,instance segmentation,object detection,scene text recognition,spatial attention module}
}

@inproceedings{wang2021scene,
  title = {Scene {{Text Recognition}} with {{Multi-decoders}}},
  booktitle = {2021 21st {{International Conference}} on {{Control}}, {{Automation}} and {{Systems}} ({{ICCAS}})},
  author = {Wang, Yao and Ha, Jong-Eun},
  year = {2021},
  month = oct,
  pages = {1523--1528},
  issn = {2642-3901},
  doi = {10.23919/ICCAS52745.2021.9649998},
  urldate = {2025-10-05},
  abstract = {In this article, we focus on the scene text recognition problem, which is one of the challenging sub-files of computer vision because of the random existence of scene text. Recently, scene text recognition has achieved state-of-art performance because of the improvement of deep learning. At present, encoder-decoder architecture was widely used for scene recognition tasks, which consist of feature extractor, sequence module. Specifically, at the decoder part, connectionist temporal classification(CTC), attention mechanism, and transformer(self-attention) are three main approaches used in recent research. CTC decoder is flexible and can handle sequences with large changes in length for its align sequences features with labels in a frame-wise manner. Attention decoder can learn better and deeper feature expression and get the better position information of each character. Attention decoder can get more robust and accurate performance for both regular and irregular scene text. Moreover, a novel decoder mechanism is introduced in our study. The proposed architecture has several advantages: the model can be trained using the end-to-end manner under the condition of multi decoders, and can deal with the sequences of arbitrary length and the images of arbitrary shape. Extensive experiments on standard benchmarks demonstrate that our model's performance is improved for regular and irregular text recognition.},
  keywords = {Attention decoder module,Computer architecture,Control systems,CTC decoder module,Deep learning,End to end frame,Feature extraction,Scene text recognition,Shape,Text recognition,Transformers}
}

@inproceedings{wang2022scene,
  title = {Scene {{Text Recognition}} with {{Multi-Encoders}}},
  booktitle = {2022 22nd {{International Conference}} on {{Control}}, {{Automation}} and {{Systems}} ({{ICCAS}})},
  author = {Wang, Yao and Ha, Jong-Eun},
  year = {2022},
  month = jan,
  pages = {1615--1620},
  issn = {2642-3901},
  doi = {10.23919/ICCAS55662.2022.10003838},
  urldate = {2025-10-05},
  abstract = {Although text recognition has significantly evolved over the years, the current models still have huge challenges, especially for irregular text images, such as complex backgrounds, curved text, diverse fonts, distortions, etc. Currently, CNN-based text recognition networks have shown good performance but still face the above challenges. Recently, feature extractor based on transformer has shown excellent advantages for global feature extraction on images. Especially in irregular text images, which can use self-attention to establish the information connection of each part of the image, which can also reduce the influence of the irregular distribution of characters. Therefore, this paper proposes MESTR(Multi-Encoders Scene Text Recognition) that combines a CNN-based [1] [2] [6] feature extractor and a transformer-based feature extractor. MESTR can extract local and global features of text images at the same time and then integrate global features into local features. During training, we used CTC [6] as guide training in the decoder part, as the compensation training strategy for attentional decoder. Experimental results demonstrate that the proposed MESTR shows competitive results on all seven benchmarks. At the same time, we provide ablation experiments to show the effectiveness of the improved part on the text recognition model.},
  keywords = {Benchmark testing,Convolutional neural network,Deep learning,Face recognition,Feature extraction,Neural networks,Scene text recognition,Text recognition,Training,Transformer,Transformers}
}

@inproceedings{wang2023detr,
  title = {{{DETR}} with {{Additional Object Instance-Specific Features}} for {{Encoder}}},
  booktitle = {2023 23rd {{International Conference}} on {{Control}}, {{Automation}} and {{Systems}} ({{ICCAS}})},
  author = {Wang, Yao and Ha, Jong-Eun},
  year = {2023},
  month = oct,
  pages = {238--240},
  issn = {2642-3901},
  doi = {10.23919/ICCAS59377.2023.10316851},
  urldate = {2025-10-05},
  abstract = {This paper focuses on the process of developing from a convolutional neural network (CNN)-based target detection method to a transformer-based DETR paradigm-based target detection method. DETR adopts a Transformer-based end-to-end detection method, and it does not use the traditional anchor box and non-maximum suppression by transforming target detection into a set prediction problem. DETR has shown competitive results on public datasets and brought new ideas and methods to the field of object detection. We observed that DETR and DETE-like models include backbone and encoder that have same effect on the image, that is, they both did the same feature extraction function. We propose to add additional embedding module, which represents the full class information, and establishes global attention between feature tokens to provide prior knowledge for the extractor.},
  keywords = {Automation,Control systems,Data mining,Deep Learning,DETR,Feature extraction,Object detection,Object Detection,Training,Transformer,Transformers}
}

@inproceedings{yeom2021hybridnet,
  title = {{{HybridNet}}: {{Indoor}} Segmentation with Range and Voxel Fusion},
  shorttitle = {{{HybridNet}}},
  booktitle = {2021 21st {{International Conference}} on {{Control}}, {{Automation}} and {{Systems}} ({{ICCAS}})},
  author = {Yeom, Sangsik and Ha, JongEun},
  year = {2021},
  month = oct,
  pages = {1509--1515},
  issn = {2642-3901},
  doi = {10.23919/ICCAS52745.2021.9649914},
  urldate = {2025-10-05},
  abstract = {In this paper, we propose a HybridNet that improves performance by fusing 2D and 3D features. A voxel-based method and a projection-based method were adopted to derive the results through one scan. Our approach consists of two parallel networks, extracts features along each dimension, and converges them in a Fusion Network. In the fusion network, the voxel blocks and 2D feature maps extracted from each structure are fused to the voxel grid and then trained through convolution. For effective training of 2D networks, we use data augmentation techniques using coordinate system rotation transformation. In addition, the performance was effectively improved by using a multi-loss with weights applied to each dimension, and better performance was achieved than the result using a single loss. Our proposed method can achieve better performance by changing the performance of the 2D network and 3D network, which can be generalized using other structures.},
  keywords = {3D Vision,Automation,Convolution,Data preprocessing,Feature extraction,PointCloud,Semantic Segmentation,Semantics,Three-dimensional displays,Training}
}

@inproceedings{yeomsangsig20203d,
  title = {{3D 공간 정보를 이용한 시맨틱 분할}},
  booktitle = {{제어로봇시스템학회 국내학술대회 논문집}},
  author = {염상식 and 하종은},
  year = {2020},
  month = jul,
  pages = {408--409},
  urldate = {2025-10-05},
  abstract = {염상식, 하종은 {\textbar} 제어로봇시스템학회 국내학술대회 논문집 {\textbar} 2020.7},
  langid = {korean},
  keywords = {PointCloud,Semantic Segmentation}
}
